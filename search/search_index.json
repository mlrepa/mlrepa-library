{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Material for MkDocs","text":""},{"location":"#welcome-to-machine-learning-repa-library","title":"Welcome to Machine Learning REPA Library","text":"<p>Welcome to the community-driven Machine Learning REPA Library, your go-to resource for MLOps and ML Engineering solutions. This library is dedicated to providing the most up-to-date and relevant information, tutorials, and solutions to help you succeed in your machine learning projects.</p> <p>We're committed to empowering the ML community with the tools and knowledge necessary to tackle even the most challenging ML projects. Join us in our mission to advance the field of ML and build a community of successful and skilled ML practitioners</p> <p>For more information about the community visit mlrepa.org.</p>"},{"location":"#repa-principles","title":"REPA principles","text":"<p>As an ML/MLOps engineer, it is crucial to embrace the REPA principles, which are fundamental to ML engineering and MLOps design. Let's explore each of these principles:</p> Principle Description Topics \u269b\ufe0f Reliable ML R:  Ensuring reliability and reproducibility of ML workflows and experiments Reproducibility, ML System Design, ML Interpretability, A/B testing, etc. \ud83e\uddea Experiments Management E: Tracking and managing experiments, including data, hyperparameters, and metrics Metrics Tracking, Experiment Versioning, Model Lifecycle Management, etc. \ud83d\udee0\ufe0f Pipelines P: Designing end-to-end ML pipelines for data ingestion, preprocessing, model training, and deployment Production ML, Continuous Training, Data Pipelines, CI/CD pipelines, Monitoring pipelines, etc. \ud83e\udd16 Automation A: Automating repetitive tasks and implementing CI/CD practices Automate ML and Data pipelines, AutoML, CI/CD, etc."},{"location":"#contributors","title":"Contributors","text":"<p>Need help?</p> <p>We hope you'll enjoyed tutorials and learn a lot of useful techniques. </p> <p>If you need any assistance, please feel free to create a GitHub issue with your question. We are always here to help and will get back to you as soon as possible. \ud83d\udc4d</p> <p>Contribute to the community! \ud83d\ude4f\ud83c\udffb</p> <p>You may be wondering how you can help the community. Here are some ways you can contribute:</p> <ul> <li>\u2b50\u00a0Put a star on our ML REPA library repository on GitHub </li> <li>\ud83d\udce3\u00a0Share our tutorials with friends and colleagues! </li> <li>\u2705 Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> <li>\u270d\ufe0f Join us to write tutorials! If you have a tutorial or just an idea, please feel free to create a GitHub issue with your suggestion. We will get back to you as soon as possible.</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d    </p>"},{"location":"#_1","title":"Material for MkDocs","text":""},{"location":"community/author-guide/","title":"Author Guide","text":""},{"location":"community/author-guide/#authors-guide","title":"Authors' Guide","text":""},{"location":"community/author-guide/#markdown-tips","title":"Markdown tips","text":""},{"location":"community/author-guide/#headings","title":"Headings","text":"<ul> <li>The page may contain only one <code>H1</code> header</li> </ul>"},{"location":"community/author-guide/#highlighting-content","title":"Highlighting content","text":"<p>Following is a list of type qualifiers provided by Material for MkDocs, whereas the default type, and thus fallback for unknown type qualifiers, is note1:</p> <p>More details on <code>mkdocs-material</code> documentation</p> <p>Note</p> <p>A piece of information the reader should take note of.</p> <p>Example</p> <p>A sample code snippet or use case to illustrate a concept or feature.</p> <p>Info</p> <p>Additional information or context that may be helpful.</p> <p>Quote</p> <p>A cited or notable statement from another source.</p> <p>Summary</p> <p>A summary of the content or key points of the tutorial or section.</p> <p>Tip</p> <p>A helpful suggestion or advice to improve the reader's understanding or workflow.</p> <p>Success</p> <p>A positive outcome or result.</p> <p>Checklist</p> <ul> <li> A positive outcome or result, done</li> <li> Incompleted item<ul> <li> Completed subitem</li> <li> Incomplete subitem</li> </ul> </li> </ul> <p>Question</p> <p>A prompt to encourage the reader to think or ask questions.</p> <p>Warning</p> <p>An alert to potential risks or issues.</p> <p>Failure</p> <p>A negative outcome or result.</p> <p>Danger</p> <p>A more severe alert to potential risks or issues.</p> <p>Bug</p> <p>A known issue or error in the software or tool being used.</p>"},{"location":"community/contribution-guide/","title":"Contribution Guide","text":"<p>Join us to write tutorials! If you have a tutorial or just an idea, please feel free to create a GitHub issue with your suggestion. We will get back to you as soon as possible.</p>"},{"location":"community/contribution-guide/#installation","title":"Installation","text":""},{"location":"community/contribution-guide/#1-fork-clone-this-repository","title":"1. Fork / Clone this repository","text":"<p>Get the evidently code example:</p> <pre><code>git clone https://github.com/mlrepa/mlrepa-library.git </code></pre>"},{"location":"community/contribution-guide/#2-create-virtual-environment","title":"2. Create virtual environment","text":"<p>Create virtual environment named <code>.venv</code> and install python libraries</p> <pre><code>python3 -m venv .venv\necho \"export PYTHONPATH=$PWD\" &gt;&gt; .venv/bin/activate\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"community/contribution-guide/#3-run-local-mkdocs-server-to-check-your-changes","title":"3. Run local mkdocs server to check your changes","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"community/contribution-guide/#4-commit-changes-and-create-a-pull-request-to-the-main-repository","title":"4. Commit changes and create a Pull Request to the main repository","text":"<p>We will get back to you as soon as possible. Looking forward to requesting from you! </p>"},{"location":"community/contribution-guide/#_1","title":"Contributors Guide","text":""},{"location":"dvc/","title":"Index","text":""},{"location":"dvc/#get-started-with-ml-engineering","title":"Get Started with ML Engineering","text":"<p>In this section, we will cover the fundamental tools and techniques for ML engineering, including the Command Line Interface (CLI), Git version control, Docker containerization, and organizing code and repositories. </p> <p>These are essential skills for building and deploying ML models, as well as collaborating with other team members on ML projects. We will cover the basics of each of these tools, including installation, setup, and usage, and provide hands-on examples to help you get started. </p> <p>Success<p>By the end of this section, you will have a strong foundation in the essential tools and techniques for ML engineering, and be ready to move on to more advanced topics.</p> </p>"},{"location":"dvc/#_1","title":"Index","text":""},{"location":"dvc/dvc-1-get-started/","title":"Get Started with DVC","text":""},{"location":"dvc/dvc-1-get-started/#get-started-with-dvc","title":"Get Started with DVC","text":""},{"location":"dvc/dvc-1-get-started/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>If you're just starting out with DVC, good examples and documentation will come in handy!</p> <p>In this tutorial, we'll go through the basics of installing and getting started with DVC! By the way, DVC has excellent documentation.</p> <p>\ud83e\uddd1\u200d\ud83d\udcbb Repository for this tutorial</p> <p>Important</p> <ul> <li>Instructions for setting up the environment are in the repository's README.</li> </ul>"},{"location":"dvc/dvc-1-get-started/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<ul> <li>Overview DVC features and benefits for ML projects</li> <li>Explain basic concepts</li> </ul>"},{"location":"dvc/dvc-1-get-started/#tutorial-get-started-with-dvc","title":"\u2692\ufe0f Tutorial: Get Started with DVC","text":""},{"location":"dvc/dvc-1-get-started/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/mlrepa/dvc-1-get-started.git\ncd dvc-1-get-started\n</code></pre>"},{"location":"dvc/dvc-1-get-started/#initialize-dvc","title":"Initialize DVC","text":"<p>At this stage, it is assumed that you have successfully:</p> <ul> <li>Cloned the repository \u2b06\ufe0f</li> <li>Installed DVC and other dependencies (see README)</li> </ul> <p>DVC requires Git to work. DVC uses Git to store metadata about data and model versions. If you have cloned the repository, Git is already set up for the tutorial. Let's assume that is the case.</p> <p>Let's create a DVC project by initializing DVC in the Git repository:</p> <pre><code>dvc init\n</code></pre> <p>A <code>.dvc</code> folder will appear in your repository. Let's take a look inside:</p> <pre><code>ls -a .dvc\n</code></pre> <p>The contents of <code>.dvc</code> will look something like this:</p> <pre><code>./      ../\n.gitignore\ncache\nconfig\ntmp\n</code></pre> <ul> <li><code>config</code> is the DVC configuration file.</li> <li><code>cache</code> is a system folder where DVC will store all the data and models that you version.</li> </ul> <p>References:</p> <ul> <li>Get Started with DVC</li> </ul>"},{"location":"dvc/dvc-1-get-started/#commit-changes","title":"Commit changes","text":"<p>After initializing DVC, you need to add the DVC project setup to Git history:</p> <pre><code>git add .\ngit commit -m \"Initialize DVC\"\n</code></pre>"},{"location":"dvc/dvc-1-get-started/#data-versioning","title":"Data Versioning","text":"<p>Run the data loading script:</p> <ul> <li>The data will be saved in <code>data/iris.csv</code>.</li> </ul> <pre><code>python src/load_data.py </code></pre> <p>You can get a list of files in the <code>data/</code> folder using the command:</p> <pre><code>du -sh data/*\n</code></pre> <p>Add the data file to DVC control</p> <pre><code>dvc add data/iris.csv\n</code></pre> <p>You can get a list of files in the <code>data/</code> folder using the command:</p> <pre><code>du -sh data/*\n</code></pre> <p>Now, a new file <code>data/iris.csv.dvc</code> will appear in the <code>data/</code> folder.</p> <ul> <li>This file contains metadata that DVC uses to track the state (versions) of the <code>data/iris.csv</code> file.</li> </ul> <pre><code>4.0K    data/iris.csv\n4.0K    data/iris.csv.dvc\n</code></pre> <p>Next, add this metadata file to the Git history:\u0414\u0430\u043b\u0435\u0435, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u044d\u0442\u043e\u0442 \u0444\u0430\u0439\u043b \u0441 \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0432 \u0438\u0441\u0442\u043e\u0440\u0438\u044e  Git: </p> <pre><code>git add .\ngit commit -m \"Add a source dataset\"\n</code></pre> <p>DVC files contain metadata about artifact versions</p> <p>Let's take a look inside:</p> <pre><code>cat data/iris.csv.dvc\n</code></pre> More details <p>Inside the generated DVC file, it stores a hash of the file with standard parameters. <code>outs</code> are the path to the file in the <code>.dvc</code> folder that we added under DVC control. DVC puts the data in the <code>.dvc/cache</code> and creates a link to this file in the working directory. This file can be added to the Git history, thus versioning it. DVC takes care of managing the actual data. The first two characters of the hash are used as a folder inside the cache, and the remaining characters are used as the name of the created file.</p> <p></p> <p>References: </p> <ul> <li>dvc.org site </li> <li>Get Started with DVC</li> </ul>"},{"location":"dvc/dvc-1-get-started/#machine-learning-pipelines","title":"Machine Learning pipelines","text":"<p>In addition to data versioning, we can create pipelines - chains of computations with dependencies between them. Here is a standard pipeline for training and evaluating a classifier:</p> <p></p> <p>We have input data that needs preprocessing, splitting into train and test sets, feature calculation, model training, and evaluation. This pipeline can be divided into separate stages. For example, we can separate the data loading and preprocessing stage, dataset splitting stage, evaluation stage, etc., and connect these chains together.</p> <p>Let's create a pipeline with 4 stages:</p> <ul> <li>load_data</li> <li>extract features</li> <li>split dataset</li> <li>train</li> <li>evaluate</li> </ul> <p>To do this, we'll use the <code>dvc stage add</code> command.</p> <p>Add a pipeline stage with <code>dvc stage add</code></p> <p>Note: earlier we added the file <code>data/iris.csv</code> under <code>DVC</code> control. To make this file output of a stage we have to remove it from <code>DVC</code> control:</p> <pre><code>dvc remove data/iris.csv.dvc\n</code></pre> <pre><code>dvc stage add \\\n-n load_data \\\n-d src/load_data.py \\\n-o data/iris.csv \\\npython src/load_data.py\n</code></pre> <p>This command adds the <code>load_data</code> stage to the pipeline. </p> <p>The stage name is <code>load_data</code>, the input dependency is the <code>src/load_data.py</code> file, the output is the <code>data/iris.csv</code> file, and the command <code>python src/load_data.py</code> is used to execute the stage.</p> <p>When you run the <code>dvc stage add</code> command, it will add the stage configuration to the <code>dvc.yaml</code> file. This file contains the pipeline configuration in YAML format, including the stages, their dependencies, outputs, and commands. You can modify this file manually to make changes to your pipeline configuration.</p> <p>To view the contents of <code>dvc.yaml</code> from the console, you can use the <code>cat</code> command:</p> <pre><code>cat dvc.yaml\n</code></pre> <p>This command will display the content of the <code>dvc.yaml</code> file in the console. If you have added only one stage so far, you will see the configuration for that stage within the file.</p>"},{"location":"dvc/dvc-1-get-started/#build-end-to-end-ml-pipeline","title":"Build end-to-end ML pipeline","text":"<p>Let\u2019s create other stages </p> <pre><code>dvc stage add \\\n-n feature_extraction \\\n-d src/featurize.py \\\n-d data/iris.csv \\\n-o data/features_iris.csv \\\npython src/featurize.py\n</code></pre> <pre><code>dvc stage add \\\n-n split_dataset \\\n-d src/split_dataset.py \\\n-d data/features_iris.csv \\\n-o data/train.csv \\\n-o data/test.csv \\\npython src/split_dataset.py --test_size 0.4\n</code></pre> <pre><code>dvc stage add \\\n-n train \\\n-d src/train.py \\\n-d data/train.csv \\\n-o data/model.joblib \\\npython src/train.py\n</code></pre> <pre><code>dvc stage add \\\n-n evaluate \\\n-d src/train.py \\\n-d src/evaluate.py \\\n-d data/test.csv \\\n-d data/model.joblib \\\n-m data/eval.txt \\\npython src/evaluate.py\n</code></pre> <p>After running all the commands, the <code>dvc.yaml</code> file will contain the complete configuration of the pipeline, which specifies the stages to be executed. </p> <p>Additionally, DVC creates another file called <code>dvc.lock</code>. This file is a technical and extended version of <code>dvc.yaml</code>, containing information about the versions of input and output dependencies (file hashes).</p> <p>When you execute the pipeline using DVC, it analyzes the hashes of the input and output dependencies and builds a Directed Acyclic Graph (DAG) of the stages. The DAG represents the order in which the stages should be executed to satisfy the dependencies. DVC uses this DAG to ensure that each stage is executed in the correct sequence and only when its dependencies are up to date.</p>"},{"location":"dvc/dvc-1-get-started/#run-and-reproduce-pipelines","title":"Run and reproduce pipelines","text":"<p>In this tutorial, we will execute the pipeline using the <code>dvc exp run</code> command </p> <pre><code>dvc exp run\n</code></pre> <p>After running the pipeline, it is necessary to commit the changes:</p> <pre><code>git add .\ngit commit -m \"Create DVC pipeline\"\n</code></pre> <p>To rerun (reproduce) the pipeline, you can use the same command. However, if you run it in the current state without any changes, DVC will not detect any modifications and will not rerun the pipeline. But if you make changes to any of the input dependencies specified with <code>-d</code>, DVC will automatically detect the change and rerun the pipeline from that point onwards. The <code>dvc exp run</code> command provides enhanced capabilities for managing and reproducing ML experiments.</p> <p>Alternatively, you can force to rerun all stages using the <code>-f</code> flag:</p> <pre><code>dvc exp run -f\n</code></pre> <p>By specifying the <code>-f</code> flag, DVC will rerun all the preceding stages and show a warning that it is removing previous versions of data that were tracked.</p> <p>Important</p> <ul> <li>Each time DVC reruns a stage, it deletes the previous cache and overwrites it to avoid duplicating data.</li> <li>At the moment of running the DVC file, it checks its hash, and if it has changed, the pipeline is rerun, overwriting all the outputs associated with that stage.</li> <li>If you want to avoid this, you need to push a specific version of the data to a remote repository beforehand.</li> </ul>"},{"location":"dvc/dvc-1-get-started/#collaborate-on-ml-experiments","title":"Collaborate on ML Experiments","text":"<p>DVC can work not only with local storage but also with remote storage repositories. By executing the <code>dvc push</code> command, DVC will send the current version of the model and data to the preconfigured remote storage repository. If your colleague clones the repository and executes the <code>dvc pull</code> command, they will obtain the specific version of the data and models that are intended for that branch. The key requirement is that everyone has access to the remote repository.</p> <p></p> <p>In this tutorial, we are simulating \"remote\" storage in the <code>tmp/dvc</code> folder. Similarly, remote storage can be set up in the cloud. </p>"},{"location":"dvc/dvc-1-get-started/#create-remote-storage-local","title":"Create remote storage (local)","text":"<ol> <li>First, create the <code>/tmp/dvc</code> folder (you can replace it with any convenient path).</li> </ol> <p><pre><code>dvc remote add -d local /tmp/dvc\n</code></pre> 2. Now we can execute <code>dvc push</code> to send the data to this storage.</p> <p><pre><code>dvc push\n</code></pre> 3. Next, let's try manually deleting the model in the project repository and \"restore\" it using DVC:</p> <pre><code>dvc pull\n</code></pre> <p>So far, we have explored three situations where DVC and its main functionality are useful:</p> <ul> <li>Data and model versioning: If you don't need pipelines or remote repositories, you can still version data for a specific project while working on your local machine. DVC enables efficient data management, even for datasets spanning several gigabytes.</li> <li>Data and model exchange among teams: Cloud-based solutions can be used to store data. This is a convenient option when you have a distributed team or face limitations on file sizes sent via email. This approach can also be used when sharing notebooks that require access to large datasets.</li> <li>Team collaboration within a large server: Teams can work with a local version of large datasets, avoiding unnecessary data transfers. Instead, they can utilize a single remote storage, which will send and store only critical versions of models or data.</li> </ul> <p>DVC provides a flexible and powerful set of features to streamline data versioning, collaboration, and pipeline management in various scenarios.</p>"},{"location":"dvc/dvc-1-get-started/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>In this tutorial, we explored the basics of using DVC (Data Version Control) to manage data and model versioning, as well as creating pipelines for machine learning projects. Here's a summary of what we covered:</p> <ol> <li>Initializing DVC: We started by initializing DVC in our project repository, which allowed us to track and manage data and model versions using Git.</li> <li>Committing Changes: After initializing DVC, we committed the DVC project setup to the Git history, ensuring that the changes are tracked and recorded.</li> <li>Data Versioning: We learned how to add data files to DVC control using the <code>dvc add</code> command. DVC tracked the metadata and versions of the data files, enabling us to easily manage and reproduce experiments.</li> <li>Creating a Pipeline: We created a pipeline with multiple stages using the <code>dvc stage add</code> command. Each stage represented a specific task or computation in the ML workflow, and we defined the dependencies between stages.</li> <li>Running and Reproducing the Pipeline: We executed the pipeline using the <code>dvc repro</code> command, which ran the stages in the correct order based on their dependencies. We also learned how to rerun the pipeline when changes occur in the input dependencies.</li> <li>Remote Storage and Collaboration: We explored the concept of remote storage in DVC, which allows us to push and pull data and models to and from a remote repository. This facilitates collaboration among team members by ensuring everyone has access to the same version of the data and models.</li> </ol> <p>DVC provides powerful capabilities for data versioning, pipeline management, and collaboration in machine learning projects. By using DVC, data scientists and ML engineers can easily track and manage their data and models, reproduce experiments, and collaborate effectively within teams. With its seamless integration with Git and support for remote storage, DVC enables efficient and reproducible machine learning workflows.</p> <p>By mastering the fundamentals covered in this tutorial, you are now equipped with the knowledge to leverage DVC effectively in your data science projects. Happy versioning and pipeline building with DVC!</p>"},{"location":"dvc/dvc-1-get-started/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ol> <li>DVC Official Documentation: The official documentation is a comprehensive resource that provides detailed information about various DVC features, commands, and best practices. It includes tutorials, examples, and reference guides.</li> <li>DVC YouTube Channel: The DVC YouTube channel hosts a collection of tutorial videos, presentations, and demos that cover different aspects of using DVC for data versioning and pipeline management. It's a great visual resource to enhance your understanding.</li> <li>Getting Started With DVC (DAGsHub)</li> </ol> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"dvc/dvc-2-data-versioning/","title":"Data Versioning with DVC","text":""},{"location":"dvc/dvc-2-data-versioning/#data-versioning-with-dvc","title":"Data Versioning with DVC","text":""},{"location":"dvc/dvc-2-data-versioning/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>This tutorial provides a detailed demonstration of approaches to data and model versioning.</p> <p>\ud83e\uddd1\u200d\ud83d\udcbb Repository for this tutorial</p> <p>Important</p> <ul> <li>Instructions for setting up the environment are in the repository's README.</li> </ul>"},{"location":"dvc/dvc-2-data-versioning/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<ul> <li>Overview Data Versioning features and use cases of DVC</li> </ul>"},{"location":"dvc/dvc-2-data-versioning/#tutorial-data-versioning-with-dvc","title":"Tutorial: Data Versioning with DVC","text":""},{"location":"dvc/dvc-2-data-versioning/#installation","title":"Installation","text":"<p>To conveniently view the structure of files and directories in the terminal, the <code>tree</code> utility can be useful. Here's how to install it on Linux:</p> <ol> <li>Open the terminal.</li> <li>Ensure that your system has internet access.</li> <li> <p>Enter the following command to install <code>tree</code>:</p> <pre><code># for Linux \nsudo apt-get install tree\n\n# for Mac\nbrew install tree\n</code></pre> </li> <li> <p>When prompted, enter the administrator (root) password.</p> </li> <li> <p>Wait for the installation to complete.</p> <p>Once the installation is finished, you can use the <code>tree</code> command to view the structure of files and directories in the terminal. Here's an example usage:</p> <pre><code>tree &lt;directory_path&gt;\n</code></pre> <p>Replace <code>&lt;directory_path&gt;</code> with the path to the directory you want to view.</p> </li> </ol>"},{"location":"dvc/dvc-2-data-versioning/#prepare-repository","title":"Prepare repository","text":"<p>To clone the example repository \"dvc-2-data-versioning,\" you can follow these steps:</p> <pre><code>git clone https://github.com/mlrepa/dvc-2-data-versioning.git\n</code></pre> <p>Checkout branch <code>tutorial</code></p> <pre><code>git checkout -b tutorial\n</code></pre> <p>Initialize DVC</p> <pre><code>dvc init\ngit add .dvc\ngit commit -m \"DVC init\"\n</code></pre>"},{"location":"dvc/dvc-2-data-versioning/#how-data-versioning-works","title":"How data versioning works?","text":"<p>To understand how data versioning works in DVC, let's perform the following steps:</p> <ol> <li> <p>Check the status of the DVC project by running the following command:</p> <p><pre><code>dvc status\n</code></pre> 2. Add a file under DVC control:</p> <p>Download a file and add it under DVC control using the <code>dvc add</code> command. The <code>-v</code> flag will display more detailed logs, allowing you to peek under the hood of what's happening.</p> <p><pre><code># Download the data file\ndvc get https://github.com/iterative/dataset-registry   get-started/data.xml -o data/data.xml\n\n# Add the data file under DVC control\ndvc add data/data.xml -v\n</code></pre> 3. Commit the changes to Git:</p> <p>After adding the file under DVC control, we need to commit the changes to Git.</p> <p><pre><code>git add data/.gitignore data/data.xml.dvc\ngit commit -m \"Add raw data\"\n</code></pre> 4. Explore the DVC cache:</p> <p>You can examine the DVC cache directory and observe the cached file.</p> <p><pre><code># Examine the DVC cache directory\nls .dvc/cache\n\n# Check the disk usage of the cached file\ndu -sh .dvc/cache/*/*\n</code></pre> 5. Examine the <code>.dvc</code> file:</p> <pre><code>cat data/data.xml.dvc\n</code></pre> </li> </ol>"},{"location":"dvc/dvc-2-data-versioning/#add-a-directory-under-version-control","title":"Add a directory under version control","text":"<p>Adding directories under version control in DVC follows a similar process as adding files. The main difference lies in how the metadata is stored, but we don't need to worry about those details as DVC takes care of it.</p> <p>Let's go through an example:</p> <pre><code>git checkout -b cats-dogs-v1\n</code></pre> <p>For this example, let's test the concept of a Data Registry. DVC allows us to use Git repositories for centralized data management. We can fetch the required version of the data by specifying a Git revision (<code>--rev</code>), such as a branch name, commit hash, or tag.</p> <pre><code># Download the cats-dogs dataset\ndvc get --rev cats-dogs-v1 \\\nhttps://github.com/iterative/dataset-registry \\\nuse-cases/cats-dogs -o datadir\n</code></pre> <p>You can explore the downloaded dataset:</p> <pre><code>ls datadir/data/train/cats\n</code></pre> <p>Now let's add the <code>datadir</code> directory under DVC control:</p> <pre><code>dvc add datadir\n</code></pre> <p>You can examine the <code>.dvc</code> file associated with the <code>datadir</code> directory:</p> <pre><code>cat datadir.dvc\n</code></pre> <p>Ensure that the changes are reflected in Git:</p> <pre><code>git status\n</code></pre> <p>To preserve the metadata in the Git history, it's crucial to make a commit:</p> <pre><code>git add .gitignore datadir.dvc\ngit commit -m \"Add datadir\"\ngit tag -a cats-dogs-v1 -m \"Create data version v1\"\n</code></pre> <p>By following these steps, you can add a directory to DVC, track its metadata, and associate it with a specific version of the data in Git.</p>"},{"location":"dvc/dvc-2-data-versioning/#tracking-changes","title":"Tracking changes","text":"<p>Track data status changes</p> <p>To track the status of the data, you can use the following command:</p> <pre><code>dvc status\n</code></pre> <p>Updating Tracking Files</p> <p>Let's switch to another branch and update the data by fetching the <code>cats-dogs-v2</code> version from the Data Registry:</p> <pre><code>git checkout -b cats-dogs-v2\n</code></pre> <p>For the sake of the experiment, let's remove the current version of the data:</p> <pre><code>rm -rf datadir/cats-dogs\n</code></pre> <p>Now, let's fetch the new data:</p> <pre><code>dvc get --rev cats-dogs-v2 \\\nhttps://github.com/iterative/dataset-registry \\\nuse-cases/cats-dogs -o datadir\n</code></pre> <p>Check the status of the tracked data:</p> <pre><code>dvc status\n</code></pre> <p>Add the updated data directory under DVC control and make a commit:</p> <pre><code>dvc add datadir\n</code></pre> <p>Check the Git status:</p> <pre><code>git status\n</code></pre> <p>Add the <code>.dvc</code> file and make a commit:</p> <pre><code>git add datadir.dvc\ngit commit -m \"Change data\"\ngit tag -a cats-dogs-v2 -m \"Create data version v2\"\n</code></pre>"},{"location":"dvc/dvc-2-data-versioning/#switching-versions","title":"Switching versions","text":"<p>Now, let's explore how to switch between different versions of the data:</p> <ul> <li><code>git checkout</code> switches to the desired Git branch.</li> <li><code>dvc checkout</code> fetches the desired version of the data.</li> </ul> <p>Checkout to the initial branch <code>tutorial</code></p> <pre><code>git checkout tutorial\ndvc checkout\n</code></pre> <pre><code># The `datadir` directory is not presented here!\nls\n</code></pre> <p>Switch to the first version of the Cats&amp;Dogs data (branch cats-dogs-v1)</p> <pre><code># Checkout to the cats-dogs-v1 branch\ngit checkout cats-dogs-v1\n</code></pre> <pre><code># Still, there is no `datadir` directory. Why?\nls\n</code></pre> <pre><code># DVC can show the status and why the 'datadir' directory is missing\ndvc status\n</code></pre> <pre><code># To bring back the 'datadir' directory, we need to execute `dvc checkout`\ndvc checkout\n</code></pre> <pre><code>ls\ndvc status\n</code></pre> <p>By following these steps, you can track changes to your data, update the tracked files, and switch between different versions of the data using Git and DVC.</p>"},{"location":"dvc/dvc-2-data-versioning/#store-and-share-data-with-remote-storage","title":"Store and share data with Remote Storage","text":"<p>Setup your remote storage (local)</p> <p>Important</p> <ul> <li>We are using the <code>/tmp</code> directory in the examples of this tutorial for simplicity purposes only.</li> <li>DO NOT use the <code>/tmp</code> directory for long-term storage of files. Your system frequently clears this directory.</li> </ul> <p>Create new remote</p> <pre><code>mkdir -p /tmp/dvc\ndvc remote add -d local /tmp/dvc\n</code></pre> <p>Note: The <code>-d</code> flag makes the local remote the default choice. </p> <p>As you can see, .dvc/config is changed</p> <pre><code>git status -s\n</code></pre> <pre><code># Check config file\ncat .dvc/config\n</code></pre> <p>Commit changes to Git</p> <pre><code>git add .\ngit commit -m \"Add remote storage\"\n</code></pre> <p>Now, you have set up a local remote storage in DVC. This allows you to push and pull data and models to and from the specified directory. </p> <p>Remember</p> <p>This is a local setup, and you should consider using remote storage solutions for real-world scenarios to ensure data durability and accessibility.</p>"},{"location":"dvc/dvc-2-data-versioning/#push-data-to-remote-storage","title":"Push data to remote storage","text":"<p>To push your data to the remote storage, use the following command:</p> <pre><code>dvc push -v\n</code></pre> <p>This command pushes the data to the remote storage specified in the DVC configuration. You can verify the changes by examining the <code>.dvc</code> file associated with the data:</p> <pre><code>cat datadir.dvc\n</code></pre> <p>Additionally, you can check the contents of the remote storage directory, <code>/tmp/dvc/42</code> in this example:</p> <pre><code>ls /tmp/dvc/42\n</code></pre>"},{"location":"dvc/dvc-2-data-versioning/#retrieve-data-from-remote-storage","title":"Retrieve data from remote storage","text":"<p>To retrieve the data from the remote storage, follow these steps:</p> <ol> <li> <p>Remove the locally cached file and data directory:</p> <p><pre><code>rm -rf .dvc/cache\nrm -rf datadir\n</code></pre> 2. Verify that the data directory is no longer present:</p> <p><pre><code>ls\n</code></pre> 3. Pull the data from the remote storage:</p> <p><pre><code>dvc pull -v\n</code></pre> 4. Verify that the data directory has been retrieved:</p> <pre><code>ls\n</code></pre> </li> </ol> <p>By performing these steps, you can push your data to a remote storage location and retrieve it whenever needed using DVC commands.</p>"},{"location":"dvc/dvc-2-data-versioning/#data-access","title":"Data Access","text":"<p>Find a dataset</p> <p>You can use the <code>dvc list</code> command to explore the DVC registry hosted on any Git server. </p> <p>For example, let's see what is available in the <code>use-cases/</code> directory of the <code>https://github.com/iterative/dataset-registry</code> repository:</p> <pre><code>dvc list https://github.com/iterative/dataset-registry use-cases\n</code></pre> <p>Download a dataset with <code>dvc get</code></p> <p>The <code>dvc get</code> command allows you to download a dataset to your working area without DVC control. It fetches the dataset from the specified location:</p> <pre><code>dvc get https://github.com/iterative/dataset-registry use-cases/cats-dogs\n</code></pre> <p>After running this command, you will see the downloaded <code>cats-dogs/</code> folder, but it is not under DVC control. There won't be a <code>cats-dogs.dvc</code> file.</p> <p>Download and track dataset <code>dvc import</code></p> <p>The <code>dvc import</code> command downloads a dataset and automatically starts tracking its version with DVC. It allows you to easily update the dataset in your project when changes are made in the Data Registry. Note that the <code>dvc import</code> command first clones the repository using SSH. Make sure to follow the instructions to set up SSH keys for your repository.</p> <pre><code>dvc import git@github.com:iterative/example-get-started data/data.xml\n</code></pre> <p>After running this command, you will see the newly downloaded <code>data.xml</code> file, and a corresponding <code>data.xml.dvc</code> file is created, indicating that DVC is tracking the data file.</p> <p>By using these commands, you can find datasets available in the DVC registry, download datasets to your working area, and add them under DVC control for versioning and management.</p>"},{"location":"dvc/dvc-2-data-versioning/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>In this tutorial, we explored the concept of data versioning using DVC (Data Version Control). We learned how to initialize DVC in a project, add files and directories under version control, track changes to the data, and switch between different versions of the data. We also saw how to set up a local remote storage and push data to it, as well as retrieve data from the remote storage. Additionally, we discovered how to access datasets from the DVC registry using commands like <code>dvc get</code> and <code>dvc import</code>.</p> <p>Data versioning with DVC provides several benefits for data scientists and machine learning engineers. It allows for easy tracking of data changes, collaboration among team members, reproducibility of experiments, and efficient management of large datasets. By combining Git and DVC, you can have a comprehensive version control system for both your code and data.</p> <p>Start using DVC in your data science projects to keep track of your data, manage different versions, and collaborate effectively with your team.</p>"},{"location":"dvc/dvc-2-data-versioning/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<p>To further enhance your understanding of data versioning with DVC, consider exploring the following resources:</p> <ol> <li>DVC Documentation: The official documentation provides comprehensive information on using DVC and its various features.</li> <li>DVC YouTube Channel: The official DVC YouTube channel hosts a collection of video tutorials and demos to deepen your knowledge of DVC.</li> <li>DVC Community Forum: Join the DVC community forum to engage with other users, ask questions, and share your experiences with DVC.</li> </ol> <p>By leveraging these additional resources, you can continue to build your expertise in data versioning with DVC and unlock its full potential for your data science projects.</p> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"dvc/dvc-3-automate-experiments/","title":"Automate ML experiments with DVC","text":""},{"location":"dvc/dvc-3-automate-experiments/#automate-ml-experiments-with-dvc","title":"Automate ML experiments with DVC","text":""},{"location":"dvc/dvc-3-automate-experiments/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>This tutorial provides a comprehensive demonstration of approaches to automate machine learning experiments using DVC (Data Version Control).</p> <p>\ud83e\uddd1\u200d\ud83d\udcbb Repository for this tutorial</p> <p>Important</p> <ul> <li>Instructions for setting up the environment are in the repository's README.</li> </ul>"},{"location":"dvc/dvc-3-automate-experiments/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<ul> <li>Learn pipelines automation features of DVC</li> </ul>"},{"location":"dvc/dvc-3-automate-experiments/#tutorial-automate-ml-experiments-with-dvc","title":"Tutorial: Automate ML experiments with DVC","text":""},{"location":"dvc/dvc-3-automate-experiments/#installation","title":"Installation","text":"<p>To conveniently view the structure of files and directories in the terminal, the <code>tree</code> utility can be useful. Here's how to install it on Linux:</p> <ol> <li>Open the terminal.</li> <li>Ensure that your system has internet access.</li> <li> <p>Enter the following command to install <code>tree</code>:</p> <pre><code># for Linux \nsudo apt-get install tree\n\n# for Mac\nbrew install tree\n</code></pre> </li> <li> <p>When prompted, enter the administrator (root) password.</p> </li> <li>Wait for the installation to complete.</li> </ol> <p>Once the installation is finished, you can use the <code>tree</code> command to view the structure of files and directories in the terminal. Here's an example usage:</p> <pre><code>tree &lt;directory_path&gt;\n</code></pre> <p>Replace <code>&lt;directory_path&gt;</code> with the path to the directory you want to view.</p>"},{"location":"dvc/dvc-3-automate-experiments/#prepare-repository","title":"Prepare repository","text":"<p>To clone the example repository, you can follow these steps:</p> <pre><code>git clone https://github.com/mlrepa/dvc-3-automate-experiments </code></pre> <p>Checkout branch <code>tutorial</code></p> <pre><code>git checkout -b tutorial\n</code></pre> <p>Initialize DVC</p> <pre><code>dvc init\ngit add .dvc\ngit commit -m \"DVC init\"\n</code></pre>"},{"location":"dvc/dvc-3-automate-experiments/#create-dvc-pipeline","title":"Create DVC pipeline","text":"<p>1 - Create <code>data_load</code> stage</p> <p>To create a <code>dvc.yaml</code> file and add the <code>data_load</code> stage to it, follow the instructions below:</p> <ol> <li>Create a new file called <code>dvc.yaml</code> in your project directory.</li> <li>Open the <code>dvc.yaml</code> file in a text editor and add the following content:</li> </ol> <pre><code>stages:\ndata_load:\ncmd: python src/data_load.py --config=params.yaml\ndeps:\n- src/data_load.py\nparams:\n- data_load\nouts:\n- data/classes.json\n- data/iris.csv\n</code></pre> <p>Explanation of the <code>data_load</code> stage configuration:</p> <ul> <li><code>data_load</code> is the name of the stage.</li> <li><code>cmd</code> specifies the command to run for this stage. In this case, it runs the <code>data_load.py</code> script with the <code>-config=params.yaml</code> argument.</li> <li><code>deps</code> lists the dependencies of this stage. Here, it includes <code>src/data_load.py</code>, indicating that if this file changes, the stage will be re-executed.</li> <li><code>params</code> specifies the parameters for this stage. In this example, it includes a single parameter called <code>data_load</code>.</li> <li><code>outs</code> lists the outputs generated by this stage. Here, it includes <code>data/classes.json</code> and <code>data/iris.csv</code>, indicating that these files will be produced as output by the stage.</li> </ul> <p>The <code>dvc.yaml</code> file is now ready with the <code>data_load</code> stage defined. You can run the pipeline or continue adding more stages.</p> <p>2 - Run the pipeline</p> <p>Run or reproduce the pipeline using the following command:</p> <pre><code>dvc exp run </code></pre> <p>This command will execute the pipeline stages defined in the <code>dvc.yaml</code> file. DVC will analyze the dependencies and execute the stages in the correct order.</p> <p>Check the size of the data directory:</p> <pre><code>du -sh data/*\n</code></pre> <p>View the directory structure:</p> <pre><code># Note: we use `tree -I ...` pattern to not list those files that match the wild-card pattern.\ntree -I .venv\n</code></pre>"},{"location":"dvc/dvc-3-automate-experiments/#build-end-to-end-machine-learning-pipeline","title":"Build end-to-end Machine Learning pipeline","text":"<p>To build an end-to-end machine learning pipeline, you need to add stages for feature extraction, dataset splitting, training, and evaluation.</p> <p>3 - Add feature extraction stage </p> <p>Update the <code>dvc.yaml</code> file with the <code>feature_extraction</code> stage:</p> <pre><code>  feature_extraction:\ncmd: python src/featurization.py --config=params.yaml\ndeps:\n- data/iris.csv\n- src/featurization.py\nparams:\n- data_load\n- featurize\nouts:\n- data/iris_featurized.csv\n</code></pre> <p>4 - Add split train/test stage</p> <p>Update the <code>dvc.yaml</code> file with the <code>split_dataset</code> stage:</p> <pre><code>  split_dataset:\ncmd: python src/split_dataset.py --config=params.yaml\ndeps:\n- data/iris_featurized.csv\n- src/split_dataset.py\nparams:\n- data_split\n- featurize\nouts:\n- data/test.csv\n- data/train.csv\n</code></pre> <p>5 - Add train stage</p> <p>Update the <code>dvc.yaml</code> file with the <code>train</code> stage:</p> <pre><code>  train:\ncmd: python src/train.py --config=params.yaml\ndeps:\n- data/train.csv\n- src/train.py\nparams:\n- data_split\n- train\nouts:\n- models/model.joblib\n</code></pre> <p>6 - Add evaluate stage</p> <p>Update the <code>dvc.yaml</code> file with the <code>evaluate</code> stage:</p> <pre><code>  evaluate:\ncmd: python src/evaluate.py --config=params.yaml\ndeps:\n- data/classes.json\n- data/test.csv\n- models/model.joblib\n- src/evaluate.py\nparams:\n- data_load\n- data_split\n- evaluate\n- train\nmetrics:\n- reports/metrics.json\nplots:\n- reports/cm.csv\n</code></pre> Final version of the <code>dvc.yaml</code> <pre><code>stages:\ndata_load:\ncmd: python src/data_load.py --config=params.yaml\ndeps:\n- src/data_load.py\nparams:\n- data_load\nouts:\n- data/classes.json\n- data/iris.csv\nfeature_extraction:\ncmd: python src/featurization.py --config=params.yaml\ndeps:\n- data/iris.csv\n- src/featurization.py\nparams:\n- data_load\n- featurize\nouts:\n- data/iris_featurized.csv\nsplit_dataset:\ncmd: python src/split_dataset.py --config=params.yaml\ndeps:\n- data/iris_featurized.csv\n- src/split_dataset.py\nparams:\n- data_split\n- featurize\nouts:\n- data/test.csv\n- data/train.csv\ntrain:\ncmd: python src/train.py --config=params.yaml\ndeps:\n- data/train.csv\n- src/train.py\nparams:\n- data_split\n- train\nouts:\n- models/model.joblib\nevaluate:\ncmd: python src/evaluate.py --config=params.yaml\ndeps:\n- data/classes.json\n- data/test.csv\n- models/model.joblib\n- src/evaluate.py\nparams:\n- data_load\n- data_split\n- evaluate\n- train\nmetrics:\n- reports/metrics.json\nplots:\n- reports/cm.csv\n</code></pre> <p>Run DVC pipeline</p> <p>Execute the following command to run the DVC pipeline:</p> <pre><code>dvc exp run </code></pre> <p>This command will execute the pipeline stages defined in the <code>dvc.yaml</code> file. DVC will analyze the dependencies and execute the stages in the correct order.</p> <p>Commit changes</p> <p>Once the pipeline execution is complete and you are satisfied with the results, commit the changes to Git using the following commands:</p> <pre><code># Commit changes\ngit add .\ngit commit -m \"Complete DVC pipeline\"\n</code></pre> <p>This will create a new commit in your Git repository, capturing the changes made during the DVC pipeline execution. It allows you to keep track of the pipeline changes and easily revert to previous versions if needed.</p>"},{"location":"dvc/dvc-3-automate-experiments/#experimenting-with-reproducible-pipelines","title":"Experimenting with reproducible pipelines","text":"<p>The <code>dvc.yaml</code> and <code>dvc.lock</code> files serve as a blueprint for your pipeline, ensuring that each time you run the pipeline, the same data and commands are used, leading to reproducible results. This is essential in machine learning and data science, as it enables you to track the evolution of your experiments and easily reproduce them when needed.</p> <p>By combining Git's version control capabilities with DVC's data versioning and pipeline management, you have a powerful solution for reproducible and collaborative data science workflows.</p>"},{"location":"dvc/dvc-3-automate-experiments/#experiment-1-add-features","title":"Experiment 1: Add features","text":"<p>In this experiment, we will add new features to the dataset by modifying the <code>featurization.py</code> file.</p> <p>First, we create a new branch called <code>exp1-ratio-features</code> to isolate the changes for this experiment:</p> <pre><code>git checkout -b exp1-ratio-features\ngit branch\n</code></pre> <p>Next, we update the <code>featurization.py</code> file. Inside the <code>get_features()</code> function, after the line <code>features = get_features(dataset)</code>, we add the following lines to calculate two new ratio features:</p> <pre><code>features['sepal_length_to_sepal_width'] = features['sepal_length'] / features['sepal_width']\nfeatures['petal_length_to_petal_width'] = features['petal_length'] / features['petal_width']\n</code></pre> <p>After making the changes, we create an experiment using the <code>dvc exp run</code> command with the name <code>-n exp1-ratio-features</code>:</p> <pre><code>dvc exp run -n exp1-ratio-features\n</code></pre> <p>To compare the metrics of this experiment with the previous pipeline run, we use the <code>dvc metrics diff --all</code> command:</p> <pre><code>dvc metrics diff --all\n</code></pre> <p>Finally, we commit the changes for this experiment:</p> <pre><code>git add .\ngit commit -m \"Experiment with new features\"\ngit tag -a \"exp1_ratio_features\" -m \"Experiment with new features\"\n</code></pre> <p>By creating a separate branch and using DVC experiments, we can easily track and manage different versions of our pipeline and evaluate the impact of specific changes on our results.</p>"},{"location":"dvc/dvc-3-automate-experiments/#experiment-2-tune-logistic-regression","title":"Experiment 2: Tune Logistic Regression","text":"<p>In this experiment, we will tune the hyperparameters of the Logistic Regression model in the <code>train.py</code> file.</p> <p>First, we create a new branch called <code>exp2-tuning-logreg</code> to isolate the changes for this experiment:</p> <pre><code>git checkout -b exp2-tuning-logreg\ngit branch\n</code></pre> <p>Since we have already checked out the code and data files in the previous experiment, there is no need to reproduce the pipeline. We can directly proceed to tuning the parameters.</p> <p>In the <code>train.py</code> file, we update the hyperparameters of the Logistic Regression model. We replace the existing parameters with the following values:</p> <pre><code>clf = LogisticRegression(\nC=0.01,\nsolver='lbfgs',\nmulti_class='multinomial',\nmax_iter=100\n)\n</code></pre> <p>Here, we specifically set the <code>C</code> parameter to 0.01.</p> <p>To create the experiment, we use the <code>dvc exp run</code> command with the name <code>-n exp2-tuning-logreg</code>:</p> <pre><code>dvc exp run -n exp2-tuning-logreg\n</code></pre> <p>We can examine the difference in metrics compared to the previous pipeline run using the following commands:</p> <pre><code>cat reports/metrics.json\n</code></pre> <pre><code>dvc metrics show\n</code></pre> <pre><code>dvc exp diff --all\n</code></pre> <p>Finally, we commit the changes for this experiment:</p> <pre><code>git add .\ngit commit -m \"Tune model. LogisticRegression. C=0.01\"\ngit tag -a \"exp2_tuning_logreg\" -m \"Tune model. LogisticRegression. C=0.01\"\n</code></pre> <p>By creating separate experiment branches and modifying the hyperparameters, we can evaluate the impact of different parameter settings on the performance of our model. This allows us to systematically explore different configurations and select the best performing model for our task.</p>"},{"location":"dvc/dvc-3-automate-experiments/#experiment-3-use-svm","title":"Experiment 3: Use SVM","text":"<p>In this experiment, we will switch from using the Logistic Regression model to the Support Vector Machine (SVM) model in the <code>train.py</code> file.</p> <p>First, we create a new branch called <code>exp3-svm</code> to isolate the changes for this experiment:</p> <pre><code>git checkout -b exp3-svm\n</code></pre> <p>Next, we update the <code>train.py</code> file by replacing the lines for the Logistic Regression model with the following lines for the SVM model:</p> <pre><code>clf = SVC(\nC=0.1,\nkernel='linear',\ngamma='scale',\ndegree=5\n)\n</code></pre> <p>Here, we set the hyperparameters for the SVM model, including the <code>C</code> parameter, the kernel type (<code>linear</code>), the gamma value (<code>scale</code>), and the degree of the polynomial kernel (5).</p> <p>To create the experiment, we use the <code>dvc exp run</code> command with the name <code>-n exp3-svm</code>:</p> <pre><code>dvc exp run -n exp3-svm\n</code></pre> <p>We can view the metrics for this experiment using the <code>dvc metrics show</code> command:</p> <pre><code>dvc metrics show\n</code></pre> <p>Finally, we commit the changes for this experiment:</p> <pre><code>git add .\ngit commit -m \"Experiment 3 with SVM estimator\"\ngit tag -a \"exp3_svm\" -m \"Experiment 3 with SVM estimator\"\n</code></pre> <p>By switching to the SVM model and evaluating its performance, we can compare it with the previous experiments using Logistic Regression. This allows us to explore different models and assess their suitability for the task at hand.</p>"},{"location":"dvc/dvc-3-automate-experiments/#compare-experiments-and-metrics-in-cli","title":"Compare experiments and metrics in CLI","text":"<p>      Source: https://iterative.ai/blog/DVC-VS-Code-extension/     </p> <p>To visualize the metrics and results of the experiments, you can use the <code>dvc exp show</code> command. This command provides a detailed overview of the experiment, including the metrics, parameters, and other relevant information.</p> <p>To save or commit the experiment and its results, you can use the <code>dvc exp push</code> command. This command saves the experiment and its associated data, metrics, and parameters, allowing you to track and share the experiment with others.</p>"},{"location":"dvc/dvc-3-automate-experiments/#use-vscode-extension-for-dvc","title":"Use VSCode extension for DVC","text":"<p>Additionally, you can refer to the DVC VS Code extension for an enhanced integration of DVC with Visual Studio Code. The extension provides a graphical interface and features to streamline the usage of DVC within the VS Code environment, making it more convenient to manage experiments and version control for your machine learning projects.</p> <p>      Source: https://iterative.ai/blog/DVC-VS-Code-extension/     </p>"},{"location":"dvc/dvc-3-automate-experiments/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>In this tutorial, we explored how to use DVC (Data Version Control) for reproducible and version-controlled machine learning pipelines. We learned about the key concepts of DVC, such as data versioning, pipeline creation, and experiment management. We saw how DVC integrates with Git to provide a comprehensive solution for data and model versioning.</p> <p>By using DVC, we can track changes to our data, create data pipelines with dependencies, and easily reproduce experiments. DVC helps in managing large datasets efficiently by storing only the differences between versions and leveraging remote storage options for collaboration and sharing.</p> <p>Throughout the tutorial, we covered the following topics:</p> <ul> <li>Setting up DVC and initializing a project</li> <li>Tracking data and models with DVC</li> <li>Creating data pipelines using DVC stages</li> <li>Running and reproducing pipelines with <code>dvc exp run</code></li> <li>Switching between different versions of data and models</li> <li>Using remote storage with DVC to store and retrieve data</li> <li>Experimenting with different pipeline configurations and hyperparameters</li> <li>Visualizing and saving experiments with DVC</li> </ul> <p>With DVC, we have a powerful tool to ensure reproducibility, collaboration, and version control in our machine learning projects. By leveraging DVC's features, we can streamline our workflow, improve experiment management, and facilitate collaboration with team members.</p>"},{"location":"dvc/dvc-3-automate-experiments/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<p>To further enhance your understanding of DVC and explore more advanced topics, consider checking out the following resources:</p> <ol> <li>DVC Documentation: The official documentation provides comprehensive information on DVC's features, concepts, and usage. It includes detailed guides, tutorials, and reference materials.</li> <li>DVC YouTube Channel: The DVC YouTube channel features a collection of video tutorials, demos, and talks on various aspects of DVC. It's a great resource to visually learn about DVC's capabilities.</li> <li>DVC Forum: The DVC Forum is a community-driven platform where you can ask questions, share ideas, and engage in discussions related to DVC. It's a valuable resource for seeking assistance and connecting with other DVC users.</li> </ol> <p>By exploring these additional resources, you can deepen your knowledge of DVC and unlock its full potential for managing data and models in your machine learning projects.</p> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"mlops/","title":"Production ML and MLOps","text":"<p>Machine Learning Engineering (MLE) involves applying scientific principles, tools, and techniques from machine learning and traditional software engineering to design and build complex computing systems. </p> <p>The Production ML  focuses on the practical aspects of deploying machine learning models into production environments. It covers the challenges and best practices involved in building, testing, deploying, and monitoring ML systems. </p> <p>MLOps emphasizes the importance of standardizing processes and technology capabilities to enable rapid and scalable deployment and operation of ML systems.</p>"},{"location":"mlops/mlops-0-overview/","title":"Intro to Production ML and MLOps","text":""},{"location":"mlops/mlops-0-overview/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>Machine Learning Engineering (MLE) involves applying scientific principles, tools, and techniques from machine learning and traditional software engineering to design and build complex computing systems. </p> <p>The Production ML  focuses on the practical aspects of deploying machine learning models into production environments. It covers the challenges and best practices involved in building, testing, deploying, and monitoring ML systems. </p> <p>MLOps emphasizes the importance of standardizing processes and technology capabilities to enable rapid and scalable deployment and operation of ML systems.</p>"},{"location":"mlops/mlops-0-overview/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<ol> <li>Understand the challenges and considerations involved in deploying ML models in production, including technical debt, data quality, and model performance.</li> <li>Learn about the software development life cycle (SDLC) and project management methodologies for effective ML development and deployment.</li> <li>Gain knowledge of DevOps principles and practices and how they can be applied to ML systems for continuous integration, delivery, and automation.</li> <li>Explore the concept of MLOps and its role in standardizing processes and technology capabilities for building, deploying, and operationalizing ML systems.</li> </ol>"},{"location":"mlops/mlops-0-overview/#production-ml","title":"\u2692\ufe0f Production ML","text":"<p>      Source: \"Hidden Technical Debt in Machine Learning Systems\" paper </p> <p>Technical Debt refers to accumulated problems in software code or architecture that arise from neglecting software quality during development, resulting in additional future work and costs.</p> <p>ML systems have additional \"opportunities\" to accumulate technical debt, and ML Engineers often face these challenges primarily.</p> <p>For example, unstable data dependencies, feedback loops, glue code, pipeline jungles, dead experimental code paths, fixed thresholds in dynamic systems, entanglement (CACE principle), etc.</p> <p>ML systems differ from other software systems in the following ways:</p> <ul> <li>Team Qualification: Data Scientists and ML Researchers may lack experience in building production services.</li> <li>Development: ML models require constant experimentation. The challenge lies in tracking and reproducing results while maintaining code flexibility and reusability.</li> <li>Testing: In addition to regular testing, we need to ensure data quality, model quality, and the adequacy of their performance.</li> <li>Deployment: ML deployment involves deploying not just the ML model but an entire ML pipeline that trains and deploys the model.</li> <li>Production: Unlike regular software, ML models become obsolete as data continuously evolves.</li> </ul>"},{"location":"mlops/mlops-0-overview/#software-development-life-cycle-sdlc-in-ml","title":"Software Development Life Cycle (SDLC) in ML","text":"<p>SDLC encompasses a set of processes that guide the entire software development journey. It starts with capturing the initial idea, documenting requirements, and progresses through various stages until the delivery of the finished software. Following the SDLC framework is essential for successful software delivery.</p> <p>      Source: Software Development Life Cycle (SDLC) </p> <p>Benefits of Applying ML in SDLC</p> <ul> <li>Project Planning and Estimation: ML can assist in project planning, scheduling, and estimation, leading to more accurate timelines and resource allocation.</li> <li>Project Progress Tracking: ML techniques can provide insights and analytics to track project progress, identify bottlenecks, and mitigate risks effectively.</li> <li>Development Speed: ML can automate repetitive tasks, improve productivity, and accelerate the development process.</li> <li>Project Management: ML-powered tools can enhance project management by streamlining workflows, facilitating collaboration, and improving communication.</li> <li>Quality Assurance: ML algorithms can be used for automated testing, identifying bugs, and ensuring software quality.</li> <li>Predictive Maintenance: ML models can be applied to monitor software performance, identify potential issues, and enable proactive maintenance.</li> <li>User Experience Enhancement: ML techniques can personalize user experiences, recommend relevant features, and improve user satisfaction.</li> <li>Decision Support: ML can analyze data and provide valuable insights to support decision-making throughout the SDLC.</li> </ul>"},{"location":"mlops/mlops-0-overview/#project-management-methodologies","title":"Project Management Methodologies","text":"<p>      Source: Waterfall or Agile </p> <p>Common project management methodologies for ML projects:</p> <ul> <li>Waterfall methodology</li> <li>Agile methodology</li> <li>Hybrid methodology</li> <li>R&amp;D methodology</li> </ul>"},{"location":"mlops/mlops-0-overview/#devops","title":"DevOps","text":"<p>DevOps is a set of practices that combines software development (Dev) and IT infrastructure and service management (Ops). The goal of DevOps is to shorten the development cycle and ensure continuous delivery of high-quality updates.</p> <p>Key DevOps Aspects</p> <ul> <li>Coding \u2013 code development and review, source code management tools, code merging.</li> <li>Building \u2013 continuous integration tools, build status.</li> <li>Testing \u2013 continuous testing tools that provide quick and timely feedback on business risks.</li> <li>Packaging \u2013 artifact repository, application pre-deployment staging.</li> <li>Releasing \u2013 change management, release approvals, release automation, continuous deployment tools.</li> <li>Configuring \u2013 infrastructure configuration and management, infrastructure as code tools.</li> <li>Monitoring \u2013 application performance monitoring, end-user experience.</li> </ul> <p>DevOps Toolchain</p> <p>      Source: What is DevOps and where is it applied </p> <p>ML and other software systems are similar in continuous integration of source control, unit testing, integration testing, and continuous delivery of software modules or packages. However, in ML, there are a few notable differences:</p> <ul> <li>CI is no longer only about testing and validating code and components, but also about testing and validating data, data schemas, and models.</li> <li>CD is no longer about a single software package or service but about a system (ML training pipeline) that should automatically deploy another service (model prediction service).</li> <li>CT is a new property unique to ML systems that focuses on automatically retraining and serving the models.</li> </ul> <p>MLOps: Continuous delivery and automation pipelines in machine learning</p>"},{"location":"mlops/mlops-0-overview/#mlops","title":"MLOps","text":"<p>Sometimes MLOps is defined as the extension of the DevOps methodology to include Machine Learning and Data Science assets as first-class citizens within the DevOps ecosystem.  However, it is important to note that MLOps encompasses a broader scope and covers various aspects of ML system development, deployment, and maintenance.</p> <p>MLOps incorporates principles, practices, and technologies that aim to streamline the entire ML lifecycle, including data preparation, model training, deployment, monitoring, and retraining. It encompasses the integration of ML workflows, models, and data pipelines into the DevOps ecosystem, treating them as first-class citizens.</p> <p>MLOps is a set of standardized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and reliably          Source: Practitioners Guide to MLOps (Google) </p> <p>What MLOps Should Do?</p> <ul> <li>Strive to unify the ML release cycle and the software application release cycle.</li> <li>Automate testing of ML artifacts.</li> <li>Apply Agile principles to ML projects.</li> <li>Integrate ML artifacts into CI/CD systems.</li> <li>Reduce technical debt associated with ML usage.</li> </ul>"},{"location":"mlops/mlops-0-overview/#mlops-level-0-manual-process","title":"MLOps Level 0: Manual Process","text":"<p>      Source: MLOps: Continuous delivery and automation pipelines in ML </p>"},{"location":"mlops/mlops-0-overview/#mlops-level-1-automation-of-ml-pipeline","title":"MLOps Level 1: Automation of ML pipeline","text":"<p>      Source: Automation of ML pipeline </p>"},{"location":"mlops/mlops-0-overview/#mlops-level-2-automation-of-cicd-pipeline","title":"MLOps Level 2: Automation of CI/CD pipeline","text":"<p>      Source: Automation of CI/CD pipeline </p>"},{"location":"mlops/mlops-0-overview/#mlops-final-pipeline","title":"MLOps - Final Pipeline","text":"<p>      Source: Final Pipeline </p>"},{"location":"mlops/mlops-0-overview/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<ul> <li>DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality.</li> <li>Continuous Integration and Continuous Delivery are among the most important practices in DevOps.</li> <li>MLOps applies the DevOps approach to the development of ML systems, taking into account their specific requirements. Continuous Training is added as a practice due to the need for model updates.</li> </ul>"},{"location":"mlops/mlops-0-overview/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ul> <li>The MLOps Blueprint</li> <li>MLOps SIG</li> <li>Practitioners Guide to MLOps (Google)</li> <li>MLOps: Continuous delivery and automation pipelines in machine learning</li> <li>A peek into Agile DevOps</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"mlops/mlops-1-system-architecture/","title":"ML Systems Architecture Design","text":""},{"location":"mlops/mlops-1-system-architecture/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>The creation and architecture of ML systems play a crucial role in the successful implementation of machine learning solutions. ML systems consist of various components, including data storage, preprocessing, model training, and prediction serving. Properly designing the architecture ensures that the system meets its intended objectives and functions effectively.</p> <p>ML systems are subject to technical debt, which refers to accumulated problems in code and architecture caused by neglecting software quality during development. ML-specific technical debt includes issues like unstable data dependencies, feedback loops, pipeline complexities, and entanglement. Addressing these challenges is essential for maintaining the system's performance and facilitating future modifications.</p> <p>Understanding ML system architecture is vital for several reasons. It provides a clear vision of the end product and guides decision-making throughout the development process. A well-designed architecture ensures system reliability, ease of maintenance, and scalability. Additionally, it promotes effective collaboration within the development team by establishing a shared understanding of the system's structure and functionality.</p>"},{"location":"mlops/mlops-1-system-architecture/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<p>The primary goals of considering ML system architecture are:</p> <ul> <li>Define a clear and comprehensive architecture that aligns with the system's objectives and requirements.</li> <li>Identify and address potential technical debt specific to ML systems to ensure long-term maintainability.</li> <li>Balance development speed and reliability by making informed trade-offs during the architectural design phase.</li> <li>Enable seamless integration with existing infrastructure and tools, leveraging established practices and frameworks.</li> <li>Facilitate efficient data preprocessing, model training, and prediction serving processes within the system.</li> <li>Ensure scalability, allowing the system to handle increasing data volumes and user demands.</li> <li>Incorporate proper monitoring and error handling mechanisms to detect and resolve issues promptly.</li> <li>Promote collaboration and shared understanding among team members by documenting and communicating the system architecture effectively.</li> </ul> <p>By achieving these goals, ML system architects can build robust and efficient systems that deliver reliable predictions and support continuous improvement and evolution of machine learning solutions.</p>"},{"location":"mlops/mlops-1-system-architecture/#creating-and-architecture-of-ml-systems","title":"\ud83c\udfd7\ufe0f Creating and Architecture of ML Systems","text":""},{"location":"mlops/mlops-1-system-architecture/#ml-solution-architecture","title":"ML Solution Architecture","text":"<p>ML Solution Architecture refers to the design and organization of components and processes within a machine learning system to create an effective and scalable solution. It involves determining the structure and interaction between various elements such as data ingestion, preprocessing, model training, evaluation, deployment, and prediction serving.</p> <p>      Source: \"Hidden Technical Debt in Machine Learning Systems\" paper </p> <p>Key components and processes:</p> <ul> <li>Data ingestion: Collecting and acquiring relevant data for training and evaluation.</li> <li>Data preprocessing: Cleaning, transforming, and preparing data for modeling.</li> <li>Feature engineering: Extracting and selecting meaningful features from the data.</li> <li>Model training: Building and training machine learning models using the prepared data.</li> <li>Model evaluation: Assessing the performance and accuracy of trained models.</li> <li>Model deployment: Integrating the models into a production environment for real-time use.</li> <li>Prediction serving: Providing predictions or recommendations based on deployed models.</li> <li>Monitoring: Continuously monitoring the system's performance and data quality.</li> <li>Scalability and efficiency: Ensuring the system can handle large-scale data and user demand.</li> <li>Security and privacy: Implementing measures to protect sensitive data and ensure compliance.</li> </ul> <p>A well-designed ML solution architecture enables the development of robust, scalable, and accurate machine learning systems. It ensures effective utilization of data, efficient model training and deployment, and reliable prediction serving.</p> <p>Why should we care about this?</p> <ol> <li>Architecture describes the end product we need to build.</li> <li>Choosing an architecture dictates the strengths, weaknesses, and future modifications of the solution.</li> <li>Poor architecture leads to technical debt and problems in its operation, maintenance, and development.</li> <li>Effective teamwork requires a shared understanding of how your system is structured.</li> </ol> <p>Technical Debt refers to the accumulated issues in software code or architecture that arise from neglecting software quality during development, resulting in additional future work and costs.</p> <p>Note</p> <p>ML Systems have additional \"capabilities\" for accumulating technical debt. ML Engineers encounter this first and foremost.</p> <p>Examples include unstable data dependencies, feedback loops, glue code, pipeline jungles, dead experimental code paths, fixed thresholds in dynamic systems, entanglement (CACE principle), and more.</p> <p>Initially, MLEs inherit all the problems and best practices from Software Engineering.</p> <p>      Source: Clean architecture </p>"},{"location":"mlops/mlops-1-system-architecture/#creating-ml-systems","title":"Creating ML Systems","text":"<p>Note</p> <p>To make great products: do machine learning like the great engineer you are, not like the great machine learning expert you aren't. \ud83d\ude04</p> <p>Source: Rules of Machine Learning</p> <p>Best Practices for ML Engineering by Martin Zinkevich:</p> <ul> <li>Make sure your pipeline is solid end to end.</li> <li>Start with a reasonable objective.</li> <li>Add common-sense features in a simple way.</li> <li>Make sure that your pipeline stays solid.</li> </ul> <p>Here are some rules for the ML system design development divided into the following phases:</p> <ol> <li>Before Machine Learning</li> <li>ML Phase I: Your First Pipeline</li> <li>ML Phase II: Feature Engineering</li> <li>ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models</li> </ol>"},{"location":"mlops/mlops-1-system-architecture/#requirements-for-architecture","title":"Requirements for Architecture","text":"<ul> <li>Requirements for feature computation<ul> <li>On-the-fly or precomputed?</li> <li>Which software tools will be used?</li> </ul> </li> <li>Requirements for training<ul> <li>How often?</li> <li>On which resources?</li> </ul> </li> <li>Requirements for prediction serving<ul> <li>How will predictions be served?</li> <li>How quickly?</li> <li>Minimum acceptable quality?</li> </ul> </li> <li>Trade-off between development speed and reliability</li> </ul> <p>Here's the information organized into a table format:</p> Requirements Important information Requirements for feature computation On-the-fly or precomputed features?  Tools/frameworks for feature computation?  Scalability/performance requirements? Requirements for training Frequency of training?  Available training resources?  Constraints/requirements for data handling? Requirements for prediction serving Method of serving predictions?  Response time requirements  Minimum acceptable prediction quality? Trade-off between development speed and reliability Development speed vs. reliability priorities  Criticality of system reliability? Requirements for autonomy Autonomous operation or human intervention?  Desired level of decision-making autonomy?  Regulatory/compliance considerations? Requirements for customizable processing Need for data processing/model training customization  Incorporating custom algorithms or pipelines?  Modular and configurable components? Requirements for handling private data Privacy regulations/concerns?  Security measures for private data?  Data handling/encryption requirements? <p>In this table, the requirements for architecture are listed in the left column, and specific details or questions related to each requirement are provided in the right column.</p>"},{"location":"mlops/mlops-1-system-architecture/#feature-preprocessing","title":"Feature Preprocessing","text":"<p>There are usually two approaches:</p> <ul> <li>Compute features in real-time</li> <li>Some features need to be precomputed</li> </ul> <p>Using existing data sources introduces integration challenges and imposes constraints, such as response time.</p> <p>A Feature Store is used to store features, which are utilized during both training and prediction serving.</p> <p>      Source: Feature store </p> <p>For example, Hopsworks.</p>"},{"location":"mlops/mlops-1-system-architecture/#training","title":"Training","text":"<ul> <li>The most common approach is scheduled retraining - running code with cron jobs or task schedulers. (This is practiced in our course)</li> <li>Another approach is online learning (e.g., using Kafka and TensorFlow).</li> </ul> <p>      Source: Feature store </p> <p>For example, Machine Learning in Production using Apache Airflow.</p>"},{"location":"mlops/mlops-1-system-architecture/#prediction-serving","title":"Prediction Serving","text":"<p>      Source: Model-as-Service </p> <ul> <li>Simple: Write a Python-based REST API wrapper (This is practiced in our course)</li> <li>Fast: Rewrite code in Go/C++ and use gRPC</li> </ul> <p>      Source: Model-as-Dependency </p> <ul> <li>In monolithic architecture applications</li> <li>In mobile applications</li> </ul> <p>      Source: Precompute serving pattern </p> <p>      Source: Model-on-Demand </p>"},{"location":"mlops/mlops-1-system-architecture/#ml-as-a-part-of-existing-services","title":"ML as a part of Existing Services","text":"<p>ML systems often arise on top of existing infrastructure:</p> <ol> <li>There is already a data warehouse.</li> <li>There is already a backend and established development approaches.</li> </ol> <p>In such cases, it is typical to use existing tools and approaches. For example, using Airflow for running model training pipelines when it is already used in the company for ETL processes. Or deploying models on AWS SageMaker/OpenShift/Heroku when developers are already familiar with these platforms.</p> <p>This can have both advantages (no need to build everything from scratch) and disadvantages (using inconvenient tools).</p>"},{"location":"mlops/mlops-1-system-architecture/#examples","title":"Examples","text":""},{"location":"mlops/mlops-1-system-architecture/#example-1-practice-in-our-course","title":"Example 1: Practice in our course","text":"<ul> <li>Task: Daily user LTV forecast to prevent churn</li> <li>Architecture:<ol> <li>Data is updated daily and available for download from Object Storage.</li> <li>Training occurs on a schedule.</li> <li>Real-time prediction using REST API queries.</li> </ol> </li> </ul>"},{"location":"mlops/mlops-1-system-architecture/#example-2","title":"Example 2","text":"<ul> <li>Requirement: Fixed frequency of response to forecasts</li> <li>Example tasks:<ul> <li>Daily user LTV forecast to prevent churn</li> <li>Hourly weather forecast</li> <li>Hourly energy consumption forecast</li> </ul> </li> <li>Architecture:<ol> <li>Data is collected from various sources and stored in a data warehouse (DWH), constantly updated.</li> <li>Scheduled training: Download fresh data (SQL), prepare the dataset, train the model.</li> <li>Scheduled prediction: Load the result into the database.</li> </ol> </li> </ul>"},{"location":"mlops/mlops-1-system-architecture/#example-3","title":"Example 3","text":"<ul> <li>Requirements: Big data, high-load services requiring a high-speed response</li> <li>Example tasks:<ul> <li>Maps: travel time from point A to point B</li> <li>Ad rotation</li> <li>Email spam classification</li> </ul> </li> <li>Architecture:<ol> <li>Data is collected from various sources (e.g., HDFS), features are computed (e.g., Spark), and stored in a Feature Store.</li> <li>Scheduled training: Download fresh data, prepare the dataset, train the model.</li> <li>Real-time prediction using gRPC queries, with code potentially rewritten in compiled languages.</li> </ol> </li> </ul>"},{"location":"mlops/mlops-1-system-architecture/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<ol> <li>Architecture refers to the description and structure of software systems.</li> <li>Technical debt is the accumulated issues in software code or architecture that arise from neglecting software quality during development, resulting in additional future work and costs.</li> <li>ML systems have their own specific technical debt, which requires special attention and effort to address, in addition to regular development technical debt.</li> <li>Discussing architecture is best started by defining the system requirements. Key ML-specific requirements include feature computation, model training, and prediction serving.</li> </ol> <p>ML systems encompass a wide range of components and considerations, from feature preprocessing to training and prediction serving. It's important to carefully design the architecture, taking into account the specific requirements of the system and considering the trade-offs between development speed, reliability, scalability, and other factors.</p>"},{"location":"mlops/mlops-1-system-architecture/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ul> <li>\"Hidden Technical Debt in Machine Learning Systems\" paper</li> <li>Rules of Machine Learning: Best Practices for ML Engineering by Martin Zinkevich</li> <li>Machine Learning Systems by Jeff Smith</li> <li>Three Levels of ML Software</li> <li>Clean Architecture: A Craftsman's Guide to Software Structure and Design, Robert C. Martin; Short video about it</li> <li>A Philosophy of Software Design (2018), John Ousterhout</li> <li>Machine Learning in Production using Apache Airflow</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"mlops/mlops-2-api-frameworks/","title":"Web Services, Frameworks, APIs","text":""},{"location":"mlops/mlops-2-api-frameworks/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>This tutorial covers the basics of RESTful APIs, including their principles and usage with HTTP methods. It introduces the cURL tool for interacting with web services and discusses the limitations of REST APIs. </p> <p>Tutorial also presents gRPC as a high-performance alternative using Protocol Buffers and the HTTP/2 protocol. OpenAPI and Flask are briefly mentioned as tools for describing REST APIs and building web applications.</p> <p>Overall, this tutorial provides a concise introduction to these concepts, exploring the fundamentals of RESTful APIs and showcasing various frameworks and tools for web development.</p>"},{"location":"mlops/mlops-2-api-frameworks/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<ul> <li>Learn the fundamentals of RESTful APIs, including their principles and usage with HTTP methods.</li> <li>Gain an understanding of the cURL tool and its application in interacting with web services.</li> <li>Explore the limitations of REST APIs and discover gRPC as a high-performance alternative.</li> <li>Understand how gRPC leverages Protocol Buffers and the HTTP/2 protocol.</li> <li>Introduction to OpenAPI and Flask as tools for describing REST APIs and building web applications.</li> <li>Develop a solid foundation in RESTful API concepts.</li> <li>Acquire knowledge of various frameworks and tools used in web development.</li> </ul>"},{"location":"mlops/mlops-2-api-frameworks/#tutorial-web-services-frameworks-apis","title":"\u2692\ufe0f Tutorial: Web Services, Frameworks, APIs","text":""},{"location":"mlops/mlops-2-api-frameworks/#api","title":"API","text":"<p>An Application Programming Interface (API) is a set of rules and protocols that allows different software applications to communicate and interact with each other. It defines the methods, data formats, and conventions that applications can use to request services from each other. </p> <p>APIs can be categorized into different types based on their purpose and implementation. Some common types include:</p> <ul> <li>Internal APIs (custom libraries) - for communication between microservices within an application/company.</li> <li>External APIs (web services) - allow third-party developers to access a service over the internet using HTTP or other protocols.</li> </ul> <p> </p> <p>How to reconcile different APIs? - REST and RPC</p>"},{"location":"mlops/mlops-2-api-frameworks/#what-is-rest","title":"What is REST?","text":"<p>REST API stands for Representational State Transfer Application Programming Interface. It is an architectural style used for designing networked applications, particularly web services. RESTful APIs are based on a set of principles and constraints that enable different systems to exchange data over the internet.</p> <p>      Source: Rest API </p> <p>So, REST is a set of principles that enable different systems to exchange data and scale applications. </p> <ol> <li>Client-Server Architecture: The client and server are separate entities that communicate over the network. The client is responsible for the user interface and user experience, while the server handles data storage and processing.</li> <li>Statelessness: Each request from the client to the server must contain all the necessary information to understand and process the request. The server does not store any client state between requests, which allows for scalability and reliability.</li> <li>Uniform Interface: REST APIs use a uniform and consistent interface for interacting with resources. This includes using standard HTTP methods (GET, POST, PUT, DELETE) for different operations on resources and utilizing unique URLs (Uniform Resource Locators) to identify resources.</li> <li>Cacheability: Responses from the server can be cached by the client, reducing the need for repeated requests to the server. Caching improves performance and efficiency, especially for resources that don't frequently change.</li> <li>Layered System: REST allows for a layered architecture, where multiple layers can exist between the client and server. Each layer adds functionality or performs specific tasks, providing flexibility and separation of concerns.</li> <li>Code-On-Demand (optional): Servers can send executable code to the client on-demand, extending the client's functionality. However, this constraint is optional and not commonly used in most REST APIs.</li> </ol> <p>A RESTful API allows performing CRUD operations on all objects represented in the system. CRUD - an abbreviation that describes four basic actions:</p> <ul> <li>C - Create</li> <li>R - Read</li> <li>U - Update</li> <li>D - Delete</li> </ul> <p>      Source: CRUD </p> <p>CRUD corresponds the four HTTP methods:</p> <ul> <li>POST \u2014 Create a new resource</li> <li>GET \u2014 Retrieve a specific resource (by id) or a collection of resources</li> <li>PUT \u2014 Update a specific resource (by id)</li> <li>DELETE \u2014 Delete a specific resource by id</li> </ul> <p>The response is usually returned in JSON or XML format (less common). Here is a general REST API model:</p> <p>      Source: REST API model </p> <p>REST API examples </p> <ul> <li>GET - Retrieve a list of objects:</li> </ul> <p>Request:</p> <pre><code>GET /api/train_samples\n</code></pre> <p>Response:</p> <pre><code>[\n{id:0, password: 'qwerty', times: 1601},\n{id:1, password: 'admin', times: 1920}\n...\n]\n</code></pre> <ul> <li>POST - Add an object:</li> </ul> <p>Request:</p> <pre><code>POST /api/train_samples/\n</code></pre> <p>Request object:</p> <pre><code>{password: '0000', times: 1000}\n</code></pre> <p>Response:</p> <pre><code>{id:9, password: '0000', times: 1000}\n</code></pre> <p>The <code>id</code> will be assigned automatically.</p> <ul> <li>PUT - Update a selected record:</li> </ul> <p>Request:</p> <pre><code>PUT /api/train_samples/1\n</code></pre> <p>Request object:</p> <pre><code>{id:1, password: 'admin', times: 2000}\n</code></pre> <p>Response:</p> <pre><code>{id:1, password: 'admin', times: 2000}\n</code></pre> <ul> <li>DELETE - Delete a selected object:</li> </ul> <p>Request:</p> <pre><code>DELETE /api/train_samples/1\n</code></pre> <p>Another example of API method description</p>"},{"location":"mlops/mlops-2-api-frameworks/#response-codes","title":"Response Codes","text":"Code Name Description 200 OK The request was successful. 201 Created Returned when a resource is created in a collection. 204 No Content There is no content. This is a response to a successful request, such as after a DELETE operation. Code Name Description 400 Bad Request Client-side error. For example, incorrect request syntax or invalid request parameters. 401 Unauthorized The client is trying to access a restricted resource without providing authorization data. 403 Forbidden The server understood the request but refuses to process it. 404 Not Found The requested resource does not exist. 405 Method Not Allowed The client attempted to use a method that is not allowed for the resource. For example, using the DELETE method, but the resource does not support it. 500 Server Error General response for a server-side error when no other error code is applicable. <p>Learn more: HTTP response status codes </p>"},{"location":"mlops/mlops-2-api-frameworks/#curl-tool","title":"cURL tool","text":"<p>cURL (pronounced \"curl\") is a command-line tool and library for making HTTP requests and interacting with web services. </p> <p>It allows you to send and receive data over various protocols, including HTTP, HTTPS, FTP, FTPS, SFTP, and more. </p> <p>Installation</p> <ul> <li>Available in MacOS, Ubuntu - accessible from the command line.</li> <li>Requires installation on Windows. Instructions. Alternatively, you can install Git Bush.</li> </ul> <p>Note</p> <p>\ud83d\udca1 To check the installation in Windows, open the command prompt cmd \u279c curl -V. If installed, you'll see a message like:</p> <pre><code>curl 7.55.1 (Windows) libcurl/7.55.1 WinSSL\n</code></pre> <p>Request Examples</p> <p>Let\u2019s look on a few examples of using cURL. You may find more examples and practice cURL on the ReqBin website, or check the cURL documentation page.</p> <ul> <li> <p>Curl - GET Request</p> <pre><code>curl &lt;https://host.com&gt;\n</code></pre> <p>GET method is the default. The same result can be obtained using:</p> <pre><code>curl -X GET &lt;https://host.com&gt;\n</code></pre> <p>To get a response with headers:</p> <pre><code>curl &lt;https://host.com&gt; -i\n</code></pre> <p>The response will contain the HTTP version, status code, and status message (e.g., HTTP/2 200 OK). Then the response headers, an empty line, and the response body.</p> </li> <li> <p>Curl - POST Request</p> <pre><code>curl -X POST &lt;https://host.com&gt;\n</code></pre> <p>Using data transfer (URL-encoded):</p> <pre><code>curl -d \"option=value_1&amp;something=value_2\"\n     -X POST &lt;https://host.com/&gt;\n</code></pre> <p>Here, <code>-d</code> or <code>--data</code> is the flag indicating data transfer.</p> </li> <li> <p>POST Request using JSON format</p> <pre><code>curl -d '{\"option\": \"val\"}' -H \"Accept:application/json\" -X POST https://host.com/\n</code></pre> <p>Here, <code>-H</code> or <code>--header</code> is the flag for request header.</p> <p>Alternatively, you can pass the JSON as a file:</p> <pre><code>curl -d \"@file.json\"  -X POST https://host.com/\n</code></pre> </li> </ul> <p>Additional flags:</p> <ul> <li><code>u user:pass</code> - for authentication when the server requires it.</li> <li><code>curl -verbose</code> - to display detailed information.</li> <li><code>L</code> - for handling redirects (if the resource has moved).</li> <li><code>O</code> - to save with the same name, or <code>o data.json</code> to specify a new name.</li> </ul>"},{"location":"mlops/mlops-2-api-frameworks/#drawbacksfeatures-of-rest-api","title":"Drawbacks/Features of REST API:","text":"<ul> <li>Need to develop language-specific APIs for each language. (Swagger can be used, which we'll cover later)</li> <li>JSON for data transfer is not a binary format. Slower data transfer but easier to view data.</li> <li>HTTP 1.1 protocol does not support streaming data transfer.</li> </ul> <p>These drawbacks are addressed by gRPC (Google Remote Procedure Call).</p>"},{"location":"mlops/mlops-2-api-frameworks/#grpc","title":"gRPC","text":"<p>gRPC (Google Remote Procedure Call) is an open-source high-performance framework developed by Google. It is designed for efficient and reliable communication between distributed systems and allows developers to define services and message types using Protocol Buffers (protobuf), a language-agnostic binary serialization format.</p> <p>gRPC is based on RPC (Remote Procedure Call) - invoking remote code on other machines.</p> <p>      Source: Remote Procedure Call </p> <p>Differences:</p> <ul> <li>Code generation using standard tools. It uses the Protoc compiler, which generates code for multiple languages, including Python.</li> <li>Binary data format Protobuf, which uses compression for faster data transfer.</li> <li>HTTP/2 protocol (since 2015) for streaming data transfer, binary format, and improved performance.</li> </ul> <p>Which one to choose?:</p> <ul> <li>For speed, choose gRPC.</li> <li>For monolithic applications accessible externally or via a browser, choose REST API.</li> <li>For distributed systems with microservices, choose gRPC.</li> <li>For streaming data (e.g., sensor data), choose gRPC.</li> </ul> <p>You can know more details here: - Quick Start and gRPC Guide for Python</p>"},{"location":"mlops/mlops-2-api-frameworks/#openapi","title":"OpenAPI","text":"<p>      Source: OpenAPI </p> <p>The widely accepted format for describing REST APIs is OpenAPI, also known as Swagger.</p> <p>The specification is a single file in JSON or YAML format, consisting of three sections:</p> <ol> <li>Header, containing the API's name, description, version, and additional information.</li> <li>Description of all resources, including their identifiers, HTTP methods, input parameters, response codes, and response body formats.</li> <li>Definitions of objects using JSON Schema format, which can be used for both input parameters and responses.</li> </ol>"},{"location":"mlops/mlops-2-api-frameworks/#web-frameworks","title":"Web Frameworks","text":""},{"location":"mlops/mlops-2-api-frameworks/#flask","title":"Flask","text":"<p>      Source: Flask </p> <p>What is it? - A web framework for Python.</p> <p>Why choose it by default?</p> <ul> <li>Minimalistic framework.</li> <li>Rapid prototyping.</li> <li>Low-level framework, which makes it easier to understand Django after learning Flask.</li> </ul> <p>Additionally, Flask is the best choice if:</p> <ul> <li>Developing a microservices architecture.</li> <li>Implementing a REST API without frontend.</li> <li>Needing flexible customization.</li> </ul>"},{"location":"mlops/mlops-2-api-frameworks/#minimal-flask-app","title":"Minimal Flask App","text":"<pre><code>from flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef hello_world():\nreturn 'Hello, World!'\nif __name__ == '__main__':\napp.run()\n</code></pre> <p>After running the application, you will see the following message:</p> <pre><code>~ python app.py\nRunning on &lt;http://127.0.0.1:5000/&gt;\n</code></pre> <p>Note</p> <p>\ud83d\udca1 Localhost - with IP address 127.0.0.1 \u279c the computer's internal network.</p>"},{"location":"mlops/mlops-2-api-frameworks/#parameters-for-apprun","title":"Parameters for app.run()","text":"<p>1. Debug mode:</p> <pre><code>app.run(debug=True)\n</code></pre> <p>Note</p> <p>The server restarts automatically when code changes.It allows working with a debugger.Remember to disable it when deploying the service.</p> <p>2. Make the server publicly accessible</p> <pre><code>app.run(host='0.0.0.0')\n</code></pre> <p>By default, it is only accessible locally.</p>"},{"location":"mlops/mlops-2-api-frameworks/#templates","title":"Templates","text":"<p>A template is a file containing HTML code and markup elements that allow for displaying dynamic content.</p> <p>The <code>render_template()</code> function invokes the Jinja2 template engine, which comes bundled with Flask.</p> <pre><code>from flask import render_template\n</code></pre> <p>Templates are stored in the <code>/templates</code> directory.</p> <p>Example of a template (<code>/templates/index.html</code>):</p> <pre><code>&lt;html&gt;\n&lt;body&gt;\n&lt;h1&gt;Prediction: {{ pred }}&lt;/h1&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Example code that renders the template into an HTML page:</p> <pre><code>from flask import Flask, render_template\napp = Flask(__name__)\n# some code\n@app.route('/')\ndef index():\nreturn render_template('index.html', pred=model.prediction)\n</code></pre>"},{"location":"mlops/mlops-2-api-frameworks/#flask-api","title":"Flask API","text":"<p>Flask-RESTX is an extension for Flask that adds support for rapid development of REST APIs.</p> <p>Documentation of Flask-RESTX</p> <p>Alternatives: flask-restplus, flask-restful</p> <p>A simple example of an application implementing an API with Flask:</p> <pre><code>from flask import Flask\nfrom flask_restx import Api, Resource, fields\napp = Flask(__name__)\napi = Api(app)\npasswords = []\na_password = api.model('Resource', {'password': fields.String})\n@api.route('/password')\nclass Prediction(Resource):\ndef get(self):\nreturn passwords\n@api.expect(a_password)\ndef post(self):\npasswords.append(api.payload)\nreturn {'Result': 'pass added'}, 201\n</code></pre> <p>Flask-RESTX provides a set of tools for generating documentation using Swagger.</p> <p>The Swagger API documentation is generated automatically and can be accessed via the root URL of the API:</p> <p>      Source: Swagger API </p>"},{"location":"mlops/mlops-2-api-frameworks/#deployment","title":"Deployment","text":"<p>Step-by-step guide on how to deploy a Flask application on Heroku</p> <p>Deployment will be covered in more detail in the 4th lecture. It will also be beneficial to have a basic understanding of Git.</p>"},{"location":"mlops/mlops-2-api-frameworks/#django","title":"Django","text":"<p>      Source: Django </p> <p>Django is another popular Python framework for web application or API development.</p> <p>Features:</p> <ul> <li>Built-in Django Admin</li> <li>Built-in protection against common vulnerabilities and attacks, such as SQL injection, CSRF, XSS, clickjacking, etc.</li> <li>ORM support</li> </ul> <p>Django is a good choice for rapid development of scalable applications. However, it may not be the best choice for microservices, simple frontend-less API applications, or database-less applications.</p>"},{"location":"mlops/mlops-2-api-frameworks/#fastapi","title":"FastAPI","text":"<p>Advantages:</p> <ul> <li>Built-in API documentation</li> <li>Asynchronous support</li> <li>Validation (with pydantic)</li> <li>High performance</li> </ul> <p>Installation:</p> <pre><code>pip install uvicorn fastapi pydantic\n</code></pre> <p>Interactive documentation</p>"},{"location":"mlops/mlops-2-api-frameworks/#streamlit","title":"Streamlit","text":"<p>      Source: Evidently AI </p> <p>Streamlit is an open-source Python framework for rapid development of machine learning dashboards without requiring frontend knowledge (HTML, CSS, and JavaScript).</p> <p>Installation</p> <pre><code>pip install streamlit\n</code></pre> <p>Features</p> <ol> <li>Widgets<ul> <li>Checkboxes</li> <li>SelectBox</li> <li>Slider</li> <li>MultiSelect (tags)</li> </ul> </li> <li>Visualization<ul> <li>Matplotlib</li> <li>Component for rendering Folium maps.</li> </ul> </li> </ol> <p>\ud83d\udd17 Useful links:</p> <ul> <li>A tutorial on building ML and data monitoring dashboards with Evidently and Streamlit</li> </ul>"},{"location":"mlops/mlops-2-api-frameworks/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<ol> <li>REST API:<ul> <li>Standardized approach for designing web services using HTTP methods and endpoints.</li> <li>Supports CRUD operations and communicates data through JSON or XML.</li> <li>Flexible and widely adopted for building web services.</li> </ul> </li> <li>gRPC:<ul> <li>High-performance framework for building distributed systems and microservices.</li> <li>Uses RPC model and Protocol Buffers for efficient communication.</li> <li>Supports multiple languages and offers advanced features like streaming.</li> </ul> </li> <li>Web Frameworks:<ul> <li>Flask, Django, and FastAPI provide structured approaches for web development.</li> <li>Flask is minimalist and suitable for prototyping.</li> <li>Django is comprehensive with built-in features like an admin interface and ORM.</li> <li>FastAPI combines performance, asynchronous processing, and automatic documentation.</li> <li>Streamlit help to develop fast prototypes of interactive UI on Python.</li> </ul> </li> </ol> <p>Overall, REST API and gRPC offer different approaches to web services, while web frameworks simplify web development with predefined structures and tools. Choose based on project requirements and desired trade-offs.</p>"},{"location":"mlops/mlops-2-api-frameworks/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ul> <li>What is a REST API? by Tom Johnson</li> <li>What is REST? by CodeAcademy</li> <li>APIs for Model Serving by Goku Mohandas</li> <li>Run Curl Commands Online</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"project/","title":"Index","text":""},{"location":"project/#get-started-with-ml-engineering","title":"Get Started with ML Engineering","text":"<p>In this section, we will cover the fundamental tools and techniques for ML engineering, including the Command Line Interface (CLI), Git version control, Docker containerization, and organizing code and repositories. </p> <p>These are essential skills for building and deploying ML models, as well as collaborating with other team members on ML projects. We will cover the basics of each of these tools, including installation, setup, and usage, and provide hands-on examples to help you get started. </p> <p>Success<p>By the end of this section, you will have a strong foundation in the essential tools and techniques for ML engineering, and be ready to move on to more advanced topics.</p> </p>"},{"location":"project/#_1","title":"Index","text":""},{"location":"project/1-cli-get-started/","title":"Command Line Interface (CLI)","text":""},{"location":"project/1-cli-get-started/#command-line-interface-cli-get-started","title":"Command Line Interface (CLI): Get Started","text":""},{"location":"project/1-cli-get-started/#overview","title":"\ud83d\udc40 Overview:","text":"<p>This tutorial will introduce you to how to use CLI to manage files and directories. </p> <p>We will cover the basic commands for navigating your file system, creating, editing, copying, renaming, moving, and deleting files and directories using the CLI.</p>"},{"location":"project/1-cli-get-started/#goals","title":"\ud83c\udfaf Goals","text":"<p>By the end of this tutorial, you should be able to:</p> <ul> <li>Understand the importance of using the CLI</li> <li>Navigate your file system using the CLI: create, edit, copy, rename, move, and delete files and directories</li> </ul>"},{"location":"project/1-cli-get-started/#tutorial-cli-get-started","title":"\u2692\ufe0f Tutorial: CLI - Get started","text":""},{"location":"project/1-cli-get-started/#1-show-files-and-directories","title":"1. Show files and directories","text":"<p>Let's start with <code>pwd</code> command. It stands for \u201cprint working directory\u201d</p> <pre><code># Show the current working directory \npwd\n</code></pre> <p>\u2026and then list the directory contents with <code>ls</code> command.</p> <pre><code># List the dir content \nls\n</code></pre> <p>The directory may have some hidden files. They are typically used to store configuration settings, preferences, etc. And hidden files aren\u2019t intended to be directly accessed or modified by the user on a regular basis.</p> <p>With the command <code>ls -a</code>, you can list all files in a directory, including hidden files.</p> <pre><code># Show all files including \u2018hidden\u2019 (whose names begin with a dot \u2018.\u2019) \nls -a </code></pre> <p>To list files in the long format, use the <code>ls</code> command with the <code>-l</code> argument.  This command's output will include information about permissions, ownership, file size, date and time of last modification, and the file or directory name.</p> <pre><code># List files in the long format\nls -l </code></pre> <p>To combine options, use <code>-la</code>. </p> <pre><code># Combine options: list all files in the long format \nls -la </code></pre> <p>To get help on <code>ls</code> command, let's use the command <code>man ls</code>. Same works for other CLI commands. </p> <p>If you need any clarification on the functionalities and options available for the <code>ls</code> command, use the command <code>man ls</code>. It stands for manual page. Same works for other CLI commands.</p> <p>To come back to the CLI, press the <code>q</code> button on the keyboard.</p> <pre><code># Show the command `Help`  (quit with `q` key)\nman ls\n</code></pre>"},{"location":"project/1-cli-get-started/#2-create-directories-and-files","title":"2. Create directories and files","text":"<p>To make a new directory in the current working directory, you should use the <code>mkdir</code> command followed by the directory name. For example, let\u2019s create <code>demo-cli</code> directory.</p> <pre><code># Make a new `demo-cli` directory \nmkdir demo-cli\n</code></pre> <p>Once the directory is created, you can move into it using the <code>cd</code> command. It stands for \u201cchange directory\u201d</p> <pre><code># Move into the `demo-cli` dir\ncd demo-cli\n</code></pre> <p>Sometimes you might need to create a new empty file. Let\u2019s create a file named <code>file.txt</code> in the current directory with <code>touch file.txt</code> command.</p> <pre><code># Create a new empty file in the working directory \ntouch file.txt\n</code></pre>"},{"location":"project/1-cli-get-started/#3-edit-files-with-the-vi-vim-editor","title":"3. Edit files with\u00a0the <code>vi</code> / <code>vim</code>\u00a0editor","text":"<p>Note</p> <p>For this learning part, you need to use a text editor. You can use Vi or Vim text editors, which are built-in CLI.</p> <p>Also, you may use another default editor on your system (like <code>nano</code>). In this case, skip this section.</p> <p>Create a file</p> <pre><code># Open a file for editing with `vi` \nvi file.txt\n\n# Open a file for editing with `vim` \nvim file.txt\n</code></pre> <p>To edit a file in vi /vim editor: </p> <ul> <li>Press <code>i</code> to switch to insert mode and add the text: <code>This is the first line</code></li> </ul> <p>In case you want to close the file without saving it: </p> <ul> <li>Press <code>Esc</code> button</li> <li>Type a string <code>:q</code> (stands for \u2018quit\u2019) and press <code>Enter</code>, you will get an expected error: <code>E37: No write since last change (add ! to override)</code></li> <li>Add <code>!</code> mark, type a string <code>:q!</code> and press <code>Enter</code>, to close the file without saving changes</li> </ul> <p>If you want to save changes made in vi / vim editor:  </p> <ul> <li>Open the file with <code>vi file.txt</code> again and add the text: <code>This is the first line</code></li> <li>Press <code>Esc</code> button</li> <li>Type a string <code>:wq</code></li> <li>Press <code>Enter</code></li> </ul> <p>Finally, to show the content of a file in CLI use the <code>cat</code> command with the file name, such as <code>file.txt</code>. This command stands for \"concatenate.\" It means that <code>cat</code> command can read and display the contents of one or multiple files to the terminal.</p> <pre><code># Show the content of the file in CLI\ncat file.txt\n</code></pre>"},{"location":"project/1-cli-get-started/#4-copy-move-and-delete-files","title":"4. Copy, move, and delete files","text":"<p>These code lines demonstrate how to copy, move, and delete files and directories using the command-line interface. Here are the steps:</p> <ol> <li>Navigate to the <code>demo-cli</code> directory using <code>cd demo-cli</code>.</li> </ol> <p><pre><code># Navigate to the `demo-cli` dir\ncd demo-cli\n</code></pre> 2. Create a copy of the <code>file.txt</code> using <code>cp file.txt file2.txt</code>.</p> <p><pre><code># Create a copy of the `file.txt` \ncp file.txt file2.txt\n</code></pre> 3. Move the copy to the parent directory (use <code>..</code>) with <code>mv file2.txt ..</code></p> <p><pre><code># Move the copy to the parent directory (us `..`)\nmv file2.txt ..\n</code></pre> 4. Navigate to the parent directory with <code>cd ..</code> command.</p> <p><pre><code># Navigate to the parent dir\ncd .. </code></pre> 5. Remove <code>file2.txt</code> using the command <code>rm</code> and the file\u2019s name such as <code>file2.txt</code></p> <p><pre><code># Remove `file2.txt` \nrm file2.txt\n</code></pre> 6. Remove the <code>demo-cli</code> directory and all its contents using <code>rm -rf demo-cli</code>.</p> <pre><code># Remove `demo-cli` dir\nrm -rf demo-cli\n</code></pre>"},{"location":"project/1-cli-get-started/#5-heat-sheet-with-commands","title":"5. \u0421heat sheet with commands","text":"<p>CLI commands</p> Command Action mkdir Creates a new folder rm Removes a file or directory cd Changes to another location in your filesystem ls Lists the files in the current directory pwd Prints working directory cat Prints in the current screen the content of a file mv Moves or rename files <p>Vi/Vim editor commands</p> Command Action :i Press i to switch to insert mode :w After making changes to a file, press [Esc] to shift to the command mode and press :w and hit [Enter] to save a file. :q To exit Vi/Vim, use the :q command and hit [Enter] :wq / :x To save a file and exit Vi/Vim simultaneously, use the :wq command and hit [Enter] or :x command :q! To force this action, use ESC and :q!"},{"location":"project/1-cli-get-started/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>Congratulations on completing this tutorial! \ud83e\udd73\u00a0 By now, you should have a solid understanding of the importance of using the CLI in machine learning development and should be able to navigate your file system using CLI commands. You have also learned how to create, edit, and delete files and directories using the CLI.</p> <p>Keep practicing and honing your skills with the CLI, and don't hesitate to explore new commands and techniques. With time and practice, you will become a CLI expert! \ud83d\ude4c\ud83c\udffb</p>"},{"location":"project/1-cli-get-started/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ul> <li>Machine Learning REPA Library</li> <li>Unix Tutorial for Beginners</li> <li>Learn Enough Command Line to Be Dangerous</li> <li>Introduction to the Unix Shell</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50\u00a0Put a star on our ML REPA library repository on GitHub </li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"project/1-git-get-started/","title":"Get started with Git","text":""},{"location":"project/1-git-get-started/#get-started-with-git","title":"Get started with Git","text":""},{"location":"project/1-git-get-started/#verview","title":"\ud83d\udc40\u00a0\u041everview","text":"<p>This tutorial provides an introduction to Git, including its basic concepts, benefits, and how it can be used for version control in Machine Learning projects.</p> <p>We will cover the basics of Git, such as creating a repository, making commits, branching, merging. Also, you can install Git and configure it for your machine at this stage. </p>"},{"location":"project/1-git-get-started/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<p>By the end of this tutorial you will: </p> <ul> <li>Understand what Git is and why it's important for ML development</li> <li>Learn the basic concepts of version control, including repositories, commits, branches, and merges</li> </ul>"},{"location":"project/1-git-get-started/#tutorial-introduction-to-git-and-version-control","title":"Tutorial: Introduction to Git and version control","text":""},{"location":"project/1-git-get-started/#1-what-is-git-and-why-is-it-important-for-ml-development","title":"1. What is Git and why is it important for ML development?","text":"<p>Git is the most popular version control system in the world. And it records the changes made to our code over time in a special database.</p> <p></p> <p>We can look at our project history and see who has made changes, what changes, when and why. And if we screw something up we can easily revert our project back to an earlier state.</p> <p>Without a version control system, we'll have to store copies of the entire project in various folders constantly. This is very slow and doesn't scale at all. Especially if multiple people have to work on the same project you would have to constantly toss around the latest code via email or some other mechanisms and then manually merge the changes.</p> <p>So, with a version control system, you can</p> <ul> <li>Keep track of your code and files changes over time. So you have a record of what has been done, who made changes and who revert to specific versions</li> <li>Collaborate easier, because Git allow merging changes of multiple people into one source</li> <li>Experiment with different features and models without losing progress</li> <li>Handle different versions of the same code at the same time</li> <li>Take different versions of the same code, compare them, and assemble them together</li> <li>Create your own copy of a collection of files to modify your copy and suggest changes</li> </ul> <p></p> <p>Why is Git important for machine learning development?</p> <p>Git as a version control system is crucial in ML development because it</p> <ul> <li>Helps avoid the pitfalls of manual version control for files, notebooks and models</li> <li>Keeps code organized and easy to manage</li> <li>Enables collaboration with other developers simultaneously without risking a loss of progress</li> <li>Allows for easy monitoring of project history, including who made what changes and when providing valuable insights into the development process.</li> </ul>"},{"location":"project/1-git-get-started/#2-basic-concepts-of-version-control-repository-commit-branch-merge-etc","title":"2. Basic concepts of version control: repository, commit, branch, merge, etc.","text":"<p>To manage changes to code over time you should learn the basic concepts of version control. Watch the video to get an understanding of how Git works:</p> <p></p>       Git explained in 100 seconds      <p>Let\u2019s summarise:</p> <ul> <li> <p>Repository is a collection of files and folders that are managed by a version control system. It contains the entire history of the project, including all changes and versions of the code.</p> </li> <li> <p>Commit is a snapshot of the changes made to the code at a particular point in time. It records the changes to the files in the repository, along with a message describing the changes.</p> </li> <li> <p>Branch.  It is a separate version of the code within the same repository, and changes made in the branch are tracked separately from the main \"master\" branch. Branches are useful for experimenting with new features or fixing bugs without affecting the main codebase.</p> </li> <li> <p>Merge. Merging is the process of combining two or more branches into a single branch. It allows developers to bring changes from one branch into another and resolve any conflicts that arise.</p> </li> <li> <p>Fork: A fork is a separate copy of the entire repository, including all its branches, commits, and history created by a user. It allows the user to experiment with the code without affecting the original repository. Forks are commonly used in open source projects, where users can contribute changes back to the original repository through pull requests.</p> </li> <li> <p>Pull/Merge request (PR/MR): A pull request is a request to merge changes from one branch into another. So you send the code updates. Then, other people look on your code, discuss it in the GitLab/ GitHub, and decide whether to merge or not</p> </li> </ul> <p>These concepts are fundamental to version control systems like Git, and understanding them is essential for effective collaboration and development in software development teams.  </p>"},{"location":"project/1-git-get-started/#3-git-installation","title":"3. Git installation","text":"<p>The first step in using Git is to install it on your machine. Here are the steps to follow:</p> <ol> <li>Go to the official Git website and download the appropriate version for your operating system.</li> <li>Follow the installation instructions provided on the Git website, or follow the videos below with the whole process of installation and configuration (for MacOS and Windows)</li> </ol>      Git installing and configuring on MacOS            Git installing and configuring on Windows"},{"location":"project/1-git-get-started/#4-git-configuration","title":"4. Git configuration","text":"<p>After installing Git, you should configure your Git settings for your machine. This includes setting your name and email address, default text editor, and merge tool using the appropriate Git commands. </p> <p>Here are the steps to follow:</p> <p>After installing Git, you should configure your Git settings for your machine. This includes setting your name and email address and setting the Git branch name to main. </p> <p>Here are the steps to follow:</p> <ol> <li>Open a terminal or command prompt window and type the following commands to set your name and email address:</li> </ol> <p><pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n</code></pre> 2. Next, set the default Git branch name to main</p> <p><pre><code>git config --global init.defaultBranch main\n</code></pre> 3. Verify that your Git settings are correct by typing the following command:</p> <pre><code>git config --list\n</code></pre>"},{"location":"project/1-git-get-started/#5-setup-ssh","title":"5. Setup SSH","text":"<p>An SSH key is an access credential for the SSH (secure shell) network protocol. Using the SSH key to your Git configuration allows you to securely authenticate and interact with remote repositories (on GitHub or GitLab) without having to repeatedly enter your username and password. </p> <ul> <li>What is a Git SSH Key?</li> </ul> <p>Generate an SSH key and add it to your Git account</p> <p>You can follow the video below to see the entire process.  Here are all the steps:</p> <ol> <li>Open a terminal or command prompt window and type the following command to generate an SSH key: <pre><code>ssh-keygen\n</code></pre></li> <li>Follow the instructions provided in the setup wizard to generate your SSH key.</li> <li> <p>Once generated, add your public SSH key to your Git account by copying the public key from the terminal or command prompt and pasting it into the appropriate field in your Git account settings.</p> <p>a. Add an SSH key to your GitLab account</p> <p>b. Add an SSH key to your GitHub account</p> </li> </ol> <p>Also, you can look at more details in the documentation to generate an SSH key pair or watch the video below.</p>      Creating and adding your SSH Key (Windows, Mac and Linux)"},{"location":"project/1-git-get-started/#conclusion","title":"\ud83c\udfc1\u00a0Conclusion","text":"<p>Congratulations on completing this tutorial! \ud83e\udd73</p> <p>By now, you should have a solid understanding of what Git is and why it is important for machine learning development.</p> <p>You have also learned the basic concepts of version control, including repository, commit, branch, merge, merge request/pull request (MR/PR), and fork.</p> <p>See you on the next step! </p>"},{"location":"project/1-git-get-started/#additional-resources","title":"\ud83c\udf93 Additional resources","text":"<ul> <li>Git documentation</li> <li>GitHub Guides</li> <li>Atlassian Git tutorial</li> <li>Git cheat sheet</li> <li>Use SSH keys to communicate with GitLab</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50\u00a0Put a star on our ML REPA library repository on GitHub </li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"project/2-git-basics/","title":"Basic Git commands","text":""},{"location":"project/2-git-basics/#basic-git-commands","title":"Basic Git commands","text":""},{"location":"project/2-git-basics/#overview","title":"\ud83d\udc40 Overview","text":"<p>In this tutorial, we will cover the basic Git commands that are essential for managing your machine learning project. </p> <p>We will learn how to create a new Git repository, how to check the status of the repository, stage changes for commit, commit changes to the repository, and how to view the commit history.</p>"},{"location":"project/2-git-basics/#goals","title":"\ud83c\udfafGoals","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>Create a new Git repository for your machine learning project</li> <li>Check the status of your repository using the <code>git status</code></li> <li>Stage changes for commit using <code>git add</code></li> <li>Commit changes to the repository using <code>git commit</code></li> <li>View the commit history using <code>git log</code></li> </ul>"},{"location":"project/2-git-basics/#tutorial-basic-git-commands","title":"\u2692\ufe0f Tutorial: Basic Git commands","text":""},{"location":"project/2-git-basics/#1-create-a-git-repository","title":"1. Create a Git repository","text":"<p>This example demonstrates the basics of using Git to create a repository and make changes to the code.</p> <p>First, create a new directory called <code>git-demo</code> using the <code>mkdir</code> command, and navigate into this directory using <code>cd</code>, all in one line.</p> <pre><code># Create a Git repo\nmkdir git-demo &amp;&amp; cd git-demo\n</code></pre> <p>Then, initialize Git in the created directory using the <code>git init</code> command.</p> <pre><code># Initiate Git repo\ngit init\n</code></pre> <p>This command creates a new empty Git repository in the current directory. Additionally, <code>git init</code> automatically creates a hidden directory called <code>.git</code>. In this directory, there are:</p> <ul> <li>staging area, a file with the information about the changes that will be included in the next commit,</li> <li>metadata and object database of your project, including version snapshots.</li> </ul> <p>These components allow Git to keep track of changes, manage branches, and facilitate version control throughout the development process.</p>"},{"location":"project/2-git-basics/#2-save-commit-changes","title":"2. Save (commit) changes","text":"<p>Next, create a new file called <code>file.txt</code> using the <code>touch</code> command, and add the string \"Hello Git!\" using the <code>echo</code> command. Print the file content to the console using the <code>cat</code> command.</p> <pre><code># Create and change a file, print its content\ntouch file.txt echo \"Hello Git\" &gt;&gt; file.txt\ncat file.txt </code></pre> <p>After making changes to the code, check the status of the Git repository with <code>git status</code>.</p> <p>Git recognizes that you created a new file and prompts you to add the file to the staging area.</p> <p>To add the file to the staging area use the <code>git add</code> command. Then, add a commit message with <code>git commit -m</code> to save the changes to the repository.</p> <pre><code># Check Git status and save updates\ngit status git add file.txt git commit -m \"My first Git commit\"\n</code></pre> <p>Finally, you can view the Git history with the <code>git log</code>, which displays a list of all the commits made to the repository, including the most recent commit with the commit message \"My first Git commit\".</p> <pre><code># View git history \ngit log  </code></pre> <p>You may find it helpful to use the <code>--oneline</code> argument when viewing logs, as it provides a more compact representation of the commit history.</p> <p>Note</p> <p>To exit the <code>git log</code> viewing session, simply press the <code>q</code> key on your keyboard.</p>"},{"location":"project/2-git-basics/#3-branching","title":"3.  Branching","text":"<p>First, you need to create a new branch within your Git repository. A branch in Git is essentially a snapshot of your code at a certain point in time that you can work on independently from other branches. Use the <code>git branch dev</code> command to create a branch called 'dev'.</p> <pre><code># Create a branch\ngit branch dev\n</code></pre> <p>Then, switch to the \"dev\" branch that you just created using the <code>git checkout dev</code> command. Now any changes you make will be made to this branch instead of the main branch.</p> <p>Alternatively, you can create a branch and switch to it at once using the <code>git checkout -b dev</code> command.</p> <pre><code># Checkout to a new branch\ngit checkout dev\n\n# Create and checkout to a new branch\ngit checkout -b dev\n</code></pre> <p>After switching to a new branch, let's make some changes. For example, append the string \"My second commit\" to the end of the file \"file.txt\" using the <code>echo \"My second commit\" &gt;&gt; file.txt</code> command. This is just an example of a change to demonstrate the concept of making changes to a branch.</p> <p>To check that you have made changes, use the <code>cat file.txt</code> command. This command simply prints out the contents of the 'file.txt' file to the terminal.</p> <p>The next important step is to add any changes you've made to the \"dev\" branch to the staging area and then commit those changes to the branch with a message that describes the changes you made. For this, use the <code>git add . &amp;&amp; git commit -m \"Update file.txt: add a new line\"</code> command.</p> <pre><code># Update and commit\necho \"My second commit\" &gt;&gt; file.txt\ncat file.txt git add . &amp;&amp; git commit -m \"Update file.txt: add a new line\"\n</code></pre> <p>Note</p> <p>Remember that to merge changes from another branch into the main branch in Git, you need to be in the main branch.</p> <p>So, don\u2019t forget to switch back to the \"main\" branch using the <code>git checkout main</code> command.</p> <p>To check that you have made changes, use the <code>cat file.txt</code> command. Notice that the changes you made on the \"dev\" branch are not reflected here yet.</p>"},{"location":"project/2-git-basics/#4-merging","title":"4. Merging","text":"<p>Merge the changes you made on the \"dev\" branch into the \"main\" branch using the <code>git merge dev</code> command. Git will automatically attempt to merge the changes together, but if there are conflicts (e.g., you made changes to the same line of code on both branches), you'll need to manually resolve them. After the merge is complete, you'll see the changes you made on the \"dev\" branch reflected on the \"main\" branch.</p> <p>And finally, print out the contents of the file \"file.txt\" on the main branch again with the <code>cat file.txt</code> command, now with the changes from the \"dev\" branch merged in.</p> <pre><code># Merge a `dev` branch to `main`\ngit checkout main cat file.txt\n\ngit merge dev\ncat file.txt\n</code></pre> <p>If you still have questions about basic Git commands we recommend you this video:</p> <p></p>      Git Tutorial for Beginners: Learn Git in 1 Hour"},{"location":"project/2-git-basics/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>Congratulations on completing this tutorial! \ud83e\udd73\u00a0</p> <p>You have learned how to create a new Git repository, how to check the status of the repository, stage changes for commit, commit changes to the repository, and how to view the commit history.</p> <p>See you soon!</p>"},{"location":"project/2-git-basics/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ul> <li>Git Tutorial for Beginners: Learn Git in 1 Hour </li> <li>Git documentation</li> <li>Using Git source control in VS Code</li> <li>Git cheat sheet</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50\u00a0Put a star on our ML REPA library repository on GitHub </li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"project/3-git-remote/","title":"Git workflow - Remote","text":""},{"location":"project/3-git-remote/#git-workflow-remote","title":"Git workflow: Remote","text":""},{"location":"project/3-git-remote/#overview","title":"\ud83d\udc40 Overview","text":"<p>In this module, you will learn how to work with a remote repository using GitHub and GitLab (look at the tutorial). </p> <p>You will clone a remote repository, update your local repository and push changes to the remote. </p> <p>Additionally, you will learn how to create a Merge Request to propose changes to the original repository.</p>"},{"location":"project/3-git-remote/#goals","title":"\ud83c\udfaf Goals","text":"<ul> <li>Clone an existing repository from the remote.</li> <li>Update the local repository with changes made on the remote repo.</li> <li>Push changes from the local repository to the remote.</li> <li>Create a Merge Request to propose changes to the original repository.</li> </ul>"},{"location":"project/3-git-remote/#tutorial-work-with-the-remote-repository-gitlab","title":"\u2692\ufe0f Tutorial: Work with the remote repository (GitLab)","text":""},{"location":"project/3-git-remote/#1-create-and-clone-a-repository","title":"1. Create and clone a repository","text":"<p>To clone a repository means to download the files from the remote repository to your computer and to create a connection between them.</p> <p>To clone a repository:</p> <ol> <li>Open a terminal and go to the directory where you want to clone the files</li> <li> <p>Copy the Git SSH URL of the repository (REPO_URL)               Copy the Git URL          </p> GitHubGitLab <p> </p> <p> </p> </li> <li> <p>Clone the repository using the command <code>git clone</code> in the CLI.</p> </li> </ol> <pre><code>git clone REPO_URL\n</code></pre> <p>Note<p>Git automatically creates a folder with the repository name and downloads the files there.</p> </p>"},{"location":"project/3-git-remote/#2-create-and-update-dev-branch-push-it-to-the-remote","title":"2. Create and Update <code>dev</code> branch, push it to the remote","text":"<p>Now let\u2019s see how to work with a remote repository using GitHub. </p> <ol> <li>First, we navigate to our 3-git-remote directory using the command <code>cd 3-git-remote</code>.</li> </ol> <p><pre><code># Go to the repository \ncd 3-git-remote\n</code></pre> 2. Then, we need to create a new branch called <code>dev</code>. In this way, you can make changes without affecting the main codebase. And we immediately switch to a new branch with the command <code>git checkout -b dev</code>.</p> <p><pre><code># Create `dev` branch and checkout to it (-b flag)\ngit checkout -b dev\n</code></pre> 3. Next, we create a new file called <code>config.yaml</code> using the <code>touch</code> command and add a line to it using <code>echo \"repo: project-1-git\" &gt;&gt; config.yaml</code>.  We confirm the changes using <code>cat config.yaml</code>.  This command will print the file\u2019s content.</p> <p><pre><code># Create a config file, make changes, confirm them\ntouch config.yaml echo \"repo: project-1-git\" &gt;&gt; config.yaml\ncat config.yaml\n</code></pre> 4. Also, we add and commit the changes to the local repository using</p> <p><pre><code># Add and commit changes\ngit add config.yaml &amp;&amp; git commit -m \"Add config.yaml\"\n</code></pre> 5. Finally, we push our changes to the remote repository (named origin) in the \u2018dev\u2019 branch using the <code>git push origin dev</code> command. </p> <pre><code># Push changes to the remote repository\ngit push origin dev\n</code></pre>"},{"location":"project/3-git-remote/#3-create-a-merge-request-dev-main","title":"3. Create a Merge Request: <code>dev</code> \u2192 <code>main</code>","text":"<p>Create a Merge Request via GitHub or GitLab  UI to the\u00a0<code>main</code>\u00a0branch</p> GitHubGitLab <p> </p> <p> </p>"},{"location":"project/3-git-remote/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>Congratulations on completing this tutorial! \ud83e\udd73</p> <p>You have learned how to work with a remote repository. You set up SSH to connect to GitHub / GitLab, cloned a remote repository, updated your local repository, and pushed changes to the remote.</p> <p>Furthermore, you learned how to create a Merge Request to propose changes to the original repository.</p>"},{"location":"project/3-git-remote/#additional-resources","title":"\ud83c\udf93 Additional Resources:","text":"<ul> <li>Git Security SSH</li> <li>GitLab documentation</li> <li>GitLab tutorials</li> <li>Git documentation</li> <li>GitHub Git cheat sheet</li> <li>Using Git source control in VS Code</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50\u00a0Put a star on our ML REPA library repository on GitHub </li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"project/4-git-fork/","title":"Git workflow - Fork","text":""},{"location":"project/4-git-fork/#git-workflow-forks","title":"Git workflow: Forks","text":""},{"location":"project/4-git-fork/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>In this tutorial, we will explore the concept of repository forks and how they are used for contributing to open-source projects.</p> <p>In addition, you will practise forking, cloning, making changes and pushing them to your remote, creating Merge Requests. Also, you will have an opportunity to understand how to handle merge conflicts</p>"},{"location":"project/4-git-fork/#goals","title":"\ud83c\udfaf Goals","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>Understand the purpose and benefits of using repository forks in collaborative projects.</li> <li>Learn the step-by-step workflow for using forks in open-source projects.</li> <li>Practice forking a project and cloning the forked repository to your local machine.</li> <li>Add your contributions to the forked project and create a Merge Request to propose changes to the original project.</li> <li>Understand how to handle merge conflicts that may arise when collaborating with Git.</li> </ul>"},{"location":"project/4-git-fork/#tutorial-work-with-repository-fork","title":"\u2692\ufe0f\u00a0Tutorial: Work with Repository Fork","text":"<p>Sometimes to contribute to other projects we need to use Forks. </p> <p>Forking allows you to create a copy of an existing repository, including all its branches, commits, and history. This is useful if you want to experiment with the project, make changes, or add features without risking the original code.</p> <p>A typical workflow for using forks in open-source projects includes the following steps:</p> <ol> <li>Fork an open-source project to create a copy of an existing project</li> <li>Clone the forked project to download and connect the files from the remote repository to your computer</li> <li>Create a branch for your changes </li> <li>Add your changes, commit and push them to the fork</li> <li>Create a Merge Request (MR) from your fork to the parent (original) project </li> </ol> <p>Let\u2019s practise it step by step.</p>"},{"location":"project/4-git-fork/#1-fork-a-project","title":"1. Fork a project","text":"<p>For learning purposes in this tutorial, we will use the following remote repository:</p> <p>Note</p> <p>https://github.com/mlrepa/mlrepa-library</p> <p>First, you need to create a fork for the training repository by following the steps:</p> <ul> <li>in the GitHub docs: Fork a repo</li> <li>in the GitLab docs:\u00a0Creating a fork</li> </ul>"},{"location":"project/4-git-fork/#2-clone-a-repo-from-your-fork","title":"2. Clone a repo from your fork","text":"<pre><code># Set a new remote\ngit clone FORK-REPO-URL\n</code></pre>"},{"location":"project/4-git-fork/#3-add-your-contribution","title":"3. Add your contribution","text":"<p>For this exercise, you may choose one of the options below: </p> <ol> <li>Vote for the next tutorial topics in the Roadmap or suggest your own (section Roadmap)</li> <li>Share the links to interesting open-source repositories of ML and AI projects that apply good engineering practices, meet REPA principles, and could be interesting to the ML REPA Community (section Projects).</li> <li>Share the links to useful resources such as tutorials, posts, or papers (section Resources)</li> </ol> <p></p> <p>Guide:</p> <p>To add your contributon you need to</p> <ol> <li>Create and switch to the branch <code>4-git-fork</code></li> <li>Add your updates to<ul> <li>Roadmap: docs/repa/roadmap.md</li> <li>Project: docs/resources/projects.md</li> <li>Resources: docs/resources/resources.md</li> </ul> </li> <li>Push updates to your fork </li> </ol>"},{"location":"project/4-git-fork/#4-create-a-merge-request","title":"4. Create a Merge Request","text":"<ul> <li>Create a Merge Request (via GitHub UI) to the\u00a0<code>main</code>\u00a0branch in the parent repository</li> </ul>"},{"location":"project/4-git-fork/#5-resolving-conflicts","title":"5. Resolving Conflicts","text":"<p>When you collaborate with Git, you may often run into merge conflicts. Conflicts can arise when multiple contributors make conflicting changes to the same file. </p> <p>Fortunately, tools like VS Code can help you identify and resolve these conflicts by highlighting the differences and providing options to accept specific changes. Once resolved, you can stag the conflicting file and commit it to ensure a smooth collaboration process.</p> <p>Please, take a look at the \"Helpful Guide to Merge Conflicts\u201d video by the VSCode team. The video:</p> <ul> <li>explains why you may get merge conflicts</li> <li>shows an approach how to resolve conflicts in VSCode IDE</li> </ul>      Helpful Guide to Merge Conflicts"},{"location":"project/4-git-fork/#conclusion","title":"\ud83c\udfc1\u00a0Conclusion","text":"<p>Congratulations on completing this tutorial! \ud83e\udd73\u00a0</p> <p>You learned about the importance of repository forks for contributing to open-source projects. You explored the step-by-step process of forking a project, cloning the forked repository to your local machine, making changes, pushing them to the remote fork, and making Merge Requests.</p> <p>You also have known how to handle merge conflicts. Good job!</p>"},{"location":"project/4-git-fork/#additional-resources","title":"\ud83c\udf93\u00a0Additional Resources","text":"<ul> <li>GitHub's documentation on creating a Merge Request</li> <li>GitLab's documentation on creating a Merge Request</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50\u00a0Put a star on our ML REPA library repository on GitHub </li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d    </p> <p></p>"},{"location":"project/project-2-virtualenv/","title":"Virtual Environments and Package Managers","text":""},{"location":"project/project-2-virtualenv/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>In this module, we will explore the concepts of virtual environments and package managers in Python. We will understand the significance of packages and modules in Python programming and learn how to leverage them effectively. Additionally, we will delve into the importance of virtual environments in software development and the benefits they offer, such as isolation, reproducibility, and version control of libraries.</p> <p>We will also explore different types of package managers and virtual environments, including popular tools like Pip, Virtualenv, Conda, Pipenv, and Poetry. Understanding the features and capabilities of these tools will enable us to choose the most suitable option for managing dependencies and creating isolated development environments.</p>"},{"location":"project/project-2-virtualenv/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<ul> <li>Understand the concepts of packages and modules in Python.</li> <li>Learn the benefits of using virtual environments in software development.</li> <li>Explore different package managers and virtual environment tools.</li> <li>Install and utilize Pipenv as a recommended package manager and virtual environment solution.</li> <li>Create and manage virtual environments using Pipenv.</li> <li>Install and manage packages within virtual environments.</li> <li>Fix dependencies and ensure reproducibility of project environments.</li> <li>Execute Python projects within virtual environments.</li> <li>Gain practical experience by completing the seminar and homework tasks.</li> </ul>"},{"location":"project/project-2-virtualenv/#tutorial","title":"\u2692\ufe0f Tutorial","text":""},{"location":"project/project-2-virtualenv/#packages-and-modules-in-python","title":"Packages and Modules in Python","text":"<ul> <li>In Python, a package is a directory that includes other directories and modules, and it also contains the <code>__init__.py</code> file. Packages are used to organize namespaces, allowing us to work with modules by specifying their level of nesting (using dot notation).</li> <li>A module is a file with the <code>.py</code> extension. Modules are used to store frequently used functions, classes, constants, etc. Modules can be categorized into two types: programs, which are intended for direct execution, and modules, which are intended for importing into other programs. It's worth noting that modules can be written not only in Python but also in other languages (e.g., C).</li> </ul>"},{"location":"project/project-2-virtualenv/#why-do-we-need-virtual-environments","title":"Why do we need virtual environments?","text":"<ul> <li>Isolation: An isolated environment prevents accidental interference with the functioning of other solutions.</li> <li>Reproducibility/Portability: A solution developed on one computer will work the same way on another computer (without errors and producing the same results).</li> <li>Version Control: Virtual environments allow us to fix the versions of the installed packages. This prevents automatic updates of packages and ensures that the project remains stable, even if newer versions of the packages are released.</li> <li>Collaboration: Virtual environments make it easier to collaborate with other developers.</li> </ul> <p>Virtual environment is an isolated Python environment that allows us to use specific libraries and their versions in our application. These settings are independent of the system settings and restrictions on which we run our application.</p> <p>A virtual environment does not fix system and driver settings; other tools, such as Docker, are used for that purpose.</p>"},{"location":"project/project-2-virtualenv/#types-of-package-managers-and-virtual-environments","title":"Types of Package Managers and Virtual Environments:","text":"<ul> <li>Pip: the most common package manager.</li> <li>Virtualenv: the foundation, the very first virtual environment.</li> <li>Conda: packages + environment + compiled images.</li> <li>Pip + Virtualenv = Pipenv (recommended option).</li> <li>Poetry: a new incarnation of Pipenv (simpler dependency management).</li> </ul> Features Pip Virtualenv Conda Pipenv Poetry Package management/installation + - + + + Virtual environments - + + + + Wheel support - - + + + Smart environment fixation - - - + + Dependency management - - - + +"},{"location":"project/project-2-virtualenv/#lifecycle-of-a-virtual-environment-using-pipenv-as-an-example","title":"Lifecycle of a Virtual Environment using <code>pipenv</code> (as an example)","text":"<ol> <li> <p>Installation (once):</p> <pre><code>pip install --user pipenv\n</code></pre> </li> <li> <p>Activating the virtual environment:</p> <pre><code>cd my_project\npipenv shell # enter the virtual environment\nexit # exit the virtual environment\n</code></pre> </li> <li> <p>Installing packages within the environment:</p> <pre><code>pipenv install pandas\n</code></pre> </li> <li> <p>Fixing the state (automatically):</p> <pre><code>pipenv install pandas==1.1.1\npipenv lock # Fixing the state\n</code></pre> </li> <li> <p>Running the project within the virtual environment:</p> <pre><code>pipenv run python your_project.py\n</code></pre> </li> </ol>"},{"location":"project/project-2-virtualenv/#locking-the-environment-with-pipenv","title":"\ud83d\udd12 Locking the Environment with Pipenv","text":"<ul> <li>Pipfile</li> </ul> <p>Example<pre><code>[[source]]\nurl = \"&lt;https://pypi.org/simple&gt;\"\nverify_ssl = true\nname = \"pypi\"\n[packages]\npandas = \"==1.1.1\"\n[dev-packages]\n[requires]\npython_version = \"3.7\"\n</code></pre> </p> <ul> <li>Pipfile.lock</li> </ul> Example <pre><code>```json\n{\n    \"_meta\": {\n        \"hash\": {\n            \"sha256\": \"8d14434df45e0ef884d6c3f6e8048ba72335637a8631cc44792f52fd20b6f97a\"\n        },\n        \"host-environment-markers\": {\n            \"implementation_name\": \"cpython\",\n            \"implementation_version\": \"3.6.1\",\n            \"os_name\": \"posix\",\n            \"platform_machine\": \"x86_64\",\n            \"platform_python_implementation\": \"CPython\",\n            \"platform_release\": \"16.7.0\",\n            \"platform_system\": \"Darwin\",\n            \"platform_version\": \"Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64\",\n            \"python_full_version\": \"3.6.1\",\n            \"python_version\": \"3.6\",\n            \"sys_platform\": \"darwin\"\n        },\n        \"pipfile-spec\": 5,\n        \"requires\": {},\n        \"sources\": [\n            {\n                \"name\": \"pypi\",\n                \"url\": \"&lt;https://pypi.python.org/simple&gt;\",\n                \"verify_ssl\": true\n            }\n        ]\n    },\n    \"default\": {\n        \"certifi\": {\n            \"hashes\": [\n                \"sha256:54a07c09c586b0e4c619f02a5e94e36619da8e2b053e20f594348c0611803704\",\n                \"sha256:40523d2efb60523e113b44602298f0960e900388cf3bb6043f645cf57ea9e3f5\"\n            ],\n            \"version\": \"==2017.7.27.1\"\n        },\n        ...\n```\n</code></pre>"},{"location":"project/project-2-virtualenv/#inside-the-pipenv-virtual-environment","title":"\ud83d\udd0d\u00a0Inside the Pipenv Virtual Environment","text":"<pre><code>pipenv graph  # View dependency graph\npipenv --venv\n</code></pre> <p>A virtual environment is physically a folder inside the project or in a specific location where all the environment's libraries are installed. This is why external changes on the host machine do not impact the project in the virtual environment and vice versa.</p>"},{"location":"project/project-2-virtualenv/#practice-pipenv-workshop","title":"\ud83d\udcbb\u00a0Practice: Pipenv Workshop","text":"<ul> <li>Install pipenv: <code>pip install --user pipenv</code></li> <li>Navigate to the project folder: <code>cd [my_project]</code></li> <li>Specify the Python version for the project: <code>pipenv --python 3.10.4</code></li> <li>Add/remove packages: <code>pipenv install/uninstall &lt;package&gt;</code></li> <li>Specify package versions: <code>pipenv install &lt;package&gt;~=1.2|&gt;=1.2|&lt;=1.2|&lt;1.2</code></li> <li>Activate the virtual environment: <code>pipenv shell | exit</code></li> <li>Lock dependencies: <code>pipenv lock</code></li> <li>Create requirements.txt: <code>pipenv lock -r &gt; requirements.txt</code></li> <li>Install packages from requirements.txt: <code>pipenv install -r requirements.txt</code></li> <li>Update dependencies: `pipenv update --outdated</li> </ul>"},{"location":"project/project-2-virtualenv/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<ul> <li>Fixing the environment saves effort and time.</li> <li>Fixing the environment is straightforward.</li> <li>We will use this approach in our projects.</li> </ul>"},{"location":"project/project-2-virtualenv/#additional-resources","title":"\ud83c\udf93\u00a0Additional Resources","text":"<ul> <li>Python Packaging User Guide This guide provides an in-depth explanation of virtual environments and package management in Python, including best practices and usage examples.</li> <li>Conda Documentation  This documentation offers a comprehensive guide on creating and managing environments using Conda, a popular package manager for Python and other languages. It covers topics such as creating, activating, and sharing environments.</li> <li>Virtualenv Documentation This documentation focuses on the usage and benefits of Virtualenv, a widely used tool for creating isolated Python environments. It provides instructions on installation, creating virtual environments, and activating them.</li> <li>Pipenv Documentation This documentation introduces Pipenv, a tool that combines package management and virtual environments in a single solution. It covers the installation process, usage instructions, and advanced features like dependency resolution.</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"project/project-4-docker/","title":"Title: Docker","text":""},{"location":"project/project-4-docker/#overview","title":"\ud83d\udc40\u00a0Overview","text":"<p>The module is designed to teach Machine Learning Engineers how to use Docker and Docker-compose, powerful tools that allow for the creation and deployment of containerized applications in a consistent and reproducible way. Learn how to use Docker to build reproducible environment, optimize Docker image size and build time, and ML models deployment.</p>"},{"location":"project/project-4-docker/#goals","title":"\ud83c\udfaf\u00a0Goals","text":"<p>Learn how to: </p> <ul> <li>Build a Docker image </li> <li>Run Docker containers </li> <li>Mount data and code into Docker container </li> <li>Build and run Docker containers with docker-compose</li> <li>Share Docker images with Docker Hub and GitLab Registry</li> <li>Optimize Docker images</li> </ul>"},{"location":"project/project-4-docker/#tutorial-docker","title":"\u2692\ufe0f Tutorial: Docker","text":"<p>Docker is a powerful tool that helps developers package their software and all its dependencies into self-contained units known as Docker containers.</p> <p>      Source: What is Docker </p> <p>Good reasons to use Docker</p> <ol> <li>Consistent and Reproducible Environments: Docker ensures that your ML models run consistently across different machines and environments.</li> <li>Easy Dependency Management: Docker simplifies managing the specific versions of software components needed for your ML project.</li> <li>Scalability and Portability: Docker containers are lightweight and portable, allowing you to easily scale and move your ML applications across different environments.</li> <li>Simplified Collaboration: Docker enables team members to work with the same environment, facilitating collaboration and accurate reproduction of ML experiments.</li> <li>Streamlined Deployment with CI/CD: Docker integrates smoothly with CI/CD pipelines, automating testing, building, and deployment of ML applications, and enhancing production deployments.</li> </ol>"},{"location":"project/project-4-docker/#key-concepts","title":"Key Concepts","text":"<p>Docker is an open platform for developing, shipping, and running applications.</p> <ul> <li>Container image (or Docker image) is a standardized unit of software that packages an application with all its dependencies - application code, runtime environment, system tools, libraries, and configurations.</li> <li>A container image becomes a container when we run it (e.g., execute <code>docker run</code>).</li> </ul> <p>Example of a Typical Folder Structure</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile    &lt;- how to create a Docker image\n\u251c\u2500\u2500 Pipfile       &lt;- pipenv\n\u251c\u2500\u2500 Pipfile.lock  &lt;- pipenv\n\u2514\u2500\u2500 app.py        &lt;- the application itself\n</code></pre>"},{"location":"project/project-4-docker/#dockerfile","title":"Dockerfile","text":"<p>A text file which contains a sequential set of all the commands or instructions that you want to include in your Docker Image of an application. Helps to:</p> <ul> <li>set the host OS (base image)</li> <li>write a list of packages you want to install</li> <li>add dependencies for your application</li> <li>set environmental variables</li> <li>provide the default command which the application will execute when the container will start executing</li> </ul> <p>Example: </p> <pre><code>FROM ubuntu:18.04\nCOPY . /app\nRUN make /app\nCMD python /app/app.py\n</code></pre> <p>Main Dockerfile statements:</p> Command Purpose FROM To specify the parent image. WORKDIR To set the working directory for any commands that follow. RUN To install applications and packages required for the container. COPY To copy files or directories from a specific location. ADD Similar to COPY, but can handle remote URLs and unpack files. ENTRYPOINT Command that always executes when the container starts. CMD Arguments passed to the entrypoint command. EXPOSE To define the port through which to access the container."},{"location":"project/project-4-docker/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub is a registry of Docker images that contains images uploaded by third-party developers as well as images released by Docker developers. Find configured Docker images for different purposes, or store your own.</p> <p>      Source: Docker Hub </p>"},{"location":"project/project-4-docker/#get-started-workflow","title":"Get Started: workflow","text":"<p>Take 10 minutes to follow the official tutorial: Get Started with Containerize an application</p> <p>      Source: Docker workflow </p> <p>Here is an explanation of the example \"Docker Hello World\":</p> <pre><code>$ docker run hello-world\n</code></pre> <p>This command pulls the \"hello-world\" image from the Docker Hub registry and runs a container based on that image. The container displays a simple message to indicate that the Docker installation is working correctly.</p> <pre><code>$ docker ps -a\n</code></pre> <p>This command lists all containers, including both running and stopped containers. Running this command after executing the previous command will show the \"hello-world\" container in the list.</p> <pre><code>$ docker stop hello-world\n</code></pre> <p>This command stops the running \"hello-world\" container by sending a termination signal. The container will transition from a running state to a stopped state.</p> <pre><code>$ docker start hello-world\n</code></pre> <p>This command starts the stopped \"hello-world\" container. The container will transition from a stopped state to a running state.</p> <p>The Docker Hello World example is a simple way to verify that your Docker installation is functioning properly. By running the container, you can confirm that Docker is able to pull images, create and manage containers, and execute applications within them.</p>"},{"location":"project/project-4-docker/#key-image-commands","title":"Key Image Commands","text":"<p>These commands are helpful for managing Docker images and containers during the development and deployment of applications.</p> <p></p> <p></p>"},{"location":"project/project-4-docker/#key-container-commands","title":"Key Container Commands","text":""},{"location":"project/project-4-docker/#container-volumes","title":"Container Volumes","text":"<p>Container Volumes provide the ability to connect a folder inside the container to a folder on the host machine.</p> <p>There are two types of Container Volumes</p> <p></p>"},{"location":"project/project-4-docker/#container-volumes_1","title":"Container Volumes","text":"<p>Creating a new Named Volume</p> <pre><code> docker volume create new-volume\n</code></pre> <p>Passing it as an argument in the <code>docker run</code> command</p> <pre><code> docker run -v new-volume:/etc/data image-name\n</code></pre>"},{"location":"project/project-4-docker/#docker-compose","title":"\ud83c\udfbc Docker Compose","text":"<p>Docker Compose is a tool that allows you to define and manage multi-container applications using a YAML file. With Docker Compose, you can easily specify the services, networks, and volumes required for your application and launch them with a single command.</p> <p>      Source: Docker Compose </p> <p>Check out an official tutorial.</p>"},{"location":"project/project-4-docker/#conclusion","title":"\ud83c\udfc1 Conclusion","text":"<p>Docker provides a powerful and efficient way to package and deploy applications, enabling consistent environments and facilitating the management of dependencies. It is widely used in the software development and deployment process, particularly in the context of microservices and containerization.</p> <p>By understanding the principles and techniques covered in this overview, you can leverage Docker to streamline your development workflow, improve scalability, and enhance the portability of your applications.</p>"},{"location":"project/project-4-docker/#additional-resources","title":"\ud83c\udf93 Additional Resources","text":"<ul> <li>Docker Documentation: The official documentation for Docker, providing comprehensive guides and references.</li> <li>Docker Hub: The central repository for Docker images, where you can find a wide range of pre-built images.</li> <li>Dockerfile Reference: The reference guide for creating Docker images using Dockerfile.</li> <li>Docker Compose: A tool for defining and running multi-container Docker applications.</li> <li>A Beginner\u2019s Guide to Understanding and Building Docker Images by Edward Kisller</li> </ul> <p>Best practices for Docker</p> <ul> <li>Best practices for writing Dockerfiles</li> <li>Docker development best practices (Docker docs)</li> <li>How to fully utilise Docker during development</li> </ul> <p>Docker Compose best practices</p> <ul> <li>Docker-compose Tricks and Best Practices</li> <li>10 Tips for Docker Compose Hosting in Production</li> <li>How to fully utilise Docker-compose during development</li> </ul> <p>*Demo projects &amp; tools*</p> <ul> <li>docker-hub-ml-project https://github.com/dockersamples/docker-hub-ml-project</li> <li>example-voting-app https://github.com/dockersamples/example-voting-app</li> <li>Dive into Docker image layers with dive: https://github.com/wagoodman/dive</li> </ul> <p>\u00a0Contribute to the community! \ud83d\ude4f\ud83c\udffb </p> <p>Hey! We hope you enjoyed the tutorial and learned a lot of useful techniques \ud83d\udd25 </p> <p>Please \ud83d\ude4f\ud83c\udffb\u00a0take a moment to improve our tutorials and create better learning experiences for the whole community. You could</p> <ul> <li>\u2b50 Put a star on our ML REPA library repository on GitHub</li> <li>\ud83d\udce3\u00a0Share our tutorials with others, and</li> <li> Fill out the Feedback Form We would appreciate any suggestions or comments you may have</li> </ul> <p>Thank you for taking the time to help the community! \ud83d\udc4d</p> <p></p>"},{"location":"repa/repa-1-principles/","title":"repa-1-principles","text":"<p>Here is the information presented in a table format:</p> Principle Description Topics : Reliable ML Ensuring reliability and reproducibility of ML workflows and experiments Reproducibility, ML System Design, ML Interpretability, A/B testing, etc. Experiments Management Tracking and managing experiments, including data, hyperparameters, and metrics Metrics Tracking, Experiment Versioning, Model Lifecycle Management, etc. Pipelines Designing end-to-end ML pipelines for data ingestion, preprocessing, model training, and deployment Production ML, Continuous Training, Data Pipelines, CI/CD pipelines, Monitoring pipelines, etc. Automation Automating repetitive tasks and implementing CI/CD practices Automate ML and Data pipelines, AutoML, CI/CD, etc. <p>As an ML/MLOps engineer, it is crucial to embrace the REPA principles, which are fundamental to ML engineering and MLOps design. Let's explore each of these principles:</p> <ol> <li>Reliable  ML:<ul> <li>Ensuring the reliability and reproducibility of ML workflows and experiments, allowing for consistent results and validation.</li> <li>Implementing robust version control and management of data, code, and configurations to enable reproducibility.</li> <li>Topics: Reproducibility,  ML System Design, ML Interpretability, A/B testing, etc.</li> </ul> </li> <li>Experiments Management:<ul> <li>Tracking and managing experiments, including data, hyperparameters, and metrics, to facilitate better experimentation and model improvement.</li> <li>Establishing standardized experiment documentation and sharing practices for collaboration and knowledge sharing.</li> <li>Topics: Metrics Tracking, Experiment Versioning, Model Lifecycle Management, etc.</li> </ul> </li> <li>Pipelines:<ul> <li>Designing end-to-end ML pipelines that encompass data ingestion, preprocessing, feature engineering, model training, evaluation, and deployment.</li> <li>Orchestrating the flow of data and tasks in the pipelines to ensure seamless integration and automation of the ML workflow.</li> <li>Topics: Production ML, Continuous Training, Data Pipelines, CI/CD pipelines, Monitoring pipelines, etc.</li> </ul> </li> <li>Automation:<ul> <li>Automating repetitive tasks such as data preprocessing, model training, and deployment to improve efficiency and reduce human errors.</li> <li>Implementing continuous integration and deployment (CI/CD) practices to automate the testing, building, and deployment of ML models.</li> <li>Topics: Automate ML and Data pipelines, AutoML, CI/CD, etc.</li> </ul> </li> </ol> <p>By embracing these principles, ML/MLOps engineers can establish robust and scalable ML engineering and MLOps workflows, leading to reliable, reproducible, and efficient machine learning systems.</p> <p>/</p>"},{"location":"repa/roadmap/","title":"Roadmap","text":"<p>Info</p> <ul> <li>Vote for the tutorial topics listed in the Roadmap by adding a <code>+1</code> to the Votes column</li> <li>Do you have a new tutorial topic in mind? Add it to the table!</li> </ul> Tutorial Tools Votes Building a Model Registry using DVC and GTO DVC, GTO 1 Data validation and Model Monitoring pipelines with DVC and Evidently DVC, Evidently 1 Model Deployment with FastAPI and Docker Docker, FastAPI 1 End-to-end pipeline to train LLM HuggingFace, LangChain 1"},{"location":"resources/projects/","title":"Projects","text":"<p>Feel free to explore these projects and contribute to the ones that align with the REPA principles and can bring value to the ML REPA Community.</p> <p>Info</p> <ul> <li>Add links to interesting open-source projects in ML and AI that apply good engineering practices, or helps to meet REPA principles</li> <li>It would be awesome, if you add your project to the list!</li> </ul> Project Description CML (Continuous Machine Learning) CI/CD for ML projects with GitHub Actions DVC (Data Version Control) Version control for ML models, data, and experiments EvidentlyAI ML model monitoring and analysis tool Great Expectations Data validation and documentation for ML pipelines Kedro Development workflow tool for reproducible ML projects Kubeflow Machine Learning toolkit for Kubernetes MLflow An open-source platform for managing the ML lifecycle"},{"location":"resources/resources/","title":"Resources","text":"<p>These resources offer tutorials, documentation, and other materials to help you dive deeper into ML and MLOps concepts, explore specific tools, and learn best practices. They will be valuable references as you continue to enhance your knowledge and skills in the field.</p> <p>Info</p> <ul> <li>Add links to interesting external resources</li> <li>It would be awesome, if you add your project to the list!</li> </ul> Resource Type Tools Batch ML monitoring blueprint: Evidently, Prefect, PostgreSQL, and Grafana Tutorial Evidently, Prefect, PostgreSQL, Grafana A tutorial on building ML and data monitoring dashboards with Evidently and Streamlit Tutorial Evidently, Streamlit"},{"location":"school/git-101-assignment-1/","title":"Assignment 1: Setting up a Git Repository for an ML Project in GitLab","text":""},{"location":"school/git-101-assignment-1/#overview","title":"\ud83d\udc40 Overview","text":"<p>This assignment is designed to provide hands-on practice with basic CLI, Git commands and GitLab. Throughout this assignment, you will clone a template repository and customize it to suit ML/AI projects.</p> <p>By the end of the assignment, you will have created a template repository similar to the one generated by Cookiecutter Data Science. This repository can be reused as a template for your future projects.</p> <p>We will be using GitLab for this assignment.</p>"},{"location":"school/git-101-assignment-1/#goals","title":"\ud83c\udfafGoals","text":"<p>Upon completing this assignment, you will be able to:</p> <ul> <li>Effectively utilize Git commands</li> <li>Practice collaborative workflows using Git and Merge Requests</li> <li>Create a template repository for ML projects</li> <li>Gain hands-on experience with GitLab</li> </ul> <p>Let's dive in and start setting up your Git repository!</p>"},{"location":"school/git-101-assignment-1/#requirements","title":"Requirements","text":"<p>To ensure the successful completion of the assignment, please make sure the following requirements are met:</p> <ul> <li> The source branch name for the Merge Request is <code>assignment</code>.</li> <li> The <code>data/</code> directory in GitLab only contains a <code>.gitignore</code> file  (files added here are NOT tracked by Git)</li> <li> The <code>models/</code> directory in GitLab has an empty <code>.gitignore</code> file (files added here are tracked by Git)</li> <li> The <code>reports/</code> directory in GitLab has an empty <code>.gitignore</code> (files added here are tracked by Git)</li> <li> The <code>notebooks/</code> directory in GitLab only contains a <code>.gitignore</code> (files added here are NOT tracked by Git)</li> <li> The root of your repository contains an empty <code>config.yaml</code> file.</li> <li> (optional) The README.md file has been updated to provide an overview of the repository, including details about its purpose, structure, and usage instructions.</li> </ul> <p>Good luck with your submission! \ud83d\ude4c\ud83c\udffb</p>"},{"location":"school/git-101-assignment-1/#assignment","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Assignment","text":""},{"location":"school/git-101-assignment-1/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Note</p> <p>\ud83d\udca1 Please, find the <code>project-1-git</code>repository in your workspace in the GitLab MLREPA Group.</p> <p>Please clone this repository with the SSH URL to your local machine.</p>"},{"location":"school/git-101-assignment-1/#2-create-a-new-branch-assignment","title":"2. Create a New Branch: <code>assignment</code>","text":"<p>Navigate to the cloned repository and create a new branch there named <code>assignment</code> and switch to it using the following command:</p> <pre><code># Create a new branch and switch to it\ngit checkout -b assignment\n</code></pre> <p>Note</p> <p>\ud83d\udca1 It\u2019s important to name the branch name exactly as an <code>assignment</code>. Other names or typos will affect the assessment results.</p>"},{"location":"school/git-101-assignment-1/#3-add-the-data-directory","title":"3. Add the \u00a0<code>data/</code>\u00a0Directory","text":"<p>In the process of working on your project, you may come across files or folders that are not meant to be included in Git version control, such as temporary files or large data files. These files can clutter your repository and make it more difficult to manage.</p> <p>To address this, Git uses a <code>.gitignore</code> file to specify files and directories that should be ignored when committing changes. Let's try it!</p> <p>Sub-task 1: Add the <code>data/</code> Directory and Exclude its Contents from Version Control</p> <p>First, inside your new <code>assignment</code> branch create an empty directory named <code>data/</code> with this command:</p> <pre><code>mkdir data\n</code></pre> <p>Next, create a <code>.gitignore</code> file in the root of your repository:</p> <pre><code>touch .gitignore\n</code></pre> <p>Update the <code>.gitignore</code> file to exclude the contents of the whole <code>data/</code> directory.</p> <p>The asterisk (*) is a wildcard character that represents any file in the specified location.</p> <pre><code># Add the following line\ndata/*\n</code></pre> <p>Commit your updates and push them in the remote repository</p> <pre><code>git add .gitignore\ngit commit -m \"Add .gitignore file\"\ngit push origin assignment\n</code></pre> <p>Note, at this point Git doesn\u2019t track any files in the <code>data/</code> directory.</p> <p>You can test this by creating a new file inside the <code>data/</code> directory and running the <code>git status</code> command. The output should not show any changes in the repository.</p> <pre><code># Creating a file in untracked directory\ntouch data/file.txt # Git Status command shouldn't show any changes in the repo\ngit status </code></pre> <p>However, if you check the remote repository on GitLab, you will notice that the <code>data/</code> directory is not present. This can become problematic if your colleagues clone your repository and expect the <code>data/</code> directory to be present.</p> <p>It is a good practice to keep the repository structure consistent between your local and remote repositories. Let's address this issue!</p> <p>Sub-task 2: Make <code>data/</code> directory visible in GitLab remote</p> <p>To make the <code>data/</code> directory visible in the GitLab remote repository without versioning all files inside it, we can create a hidden file inside the directory and instruct Git to track only that single file.</p> <p>First, let's create an empty <code>.gitignore</code> file in the <code>data/</code> directory:</p> <pre><code># Create \".gitignore\" file in \"data/\" directory\ntouch data/.gitignore\n</code></pre> <p>Next, add <code>data/.gitignore</code> to Git by appending the following line to the file:</p> <pre><code>echo '!.gitignore' &gt;&gt; data/.gitignore\n</code></pre> <p>Then, add <code>data/.gitignore</code> to Git using the force mode. </p> <p>By using the <code>-f</code> flag, you're instructing Git to forcefully add the <code>.gitignore</code> file to the staging area, regardless of any ignore rules defined.</p> <pre><code>git add data/.gitignore -f\n</code></pre> <p>Commit and push the updates to the remote repository:</p> <pre><code>git commit -m \"Add `data/.gitignore` file\"\ngit push origin assignment\n</code></pre> <p>By following these steps, you will achieve the following expected results:</p> <ul> <li>The <code>file.txt</code> inside the <code>data/</code> directory will not be added to the Git history.</li> <li>The <code>data/</code> directory will be visible in the GitLab remote repository.</li> </ul> <p>This approach allows you to exclude specific files from version control while still making the directory itself visible in the remote repository. It helps maintain consistency between your local and remote repositories and ensures that your colleagues can access and use the necessary directory structure for your ML project.</p>"},{"location":"school/git-101-assignment-1/#4-add-models-directory","title":"4. Add <code>models/</code> directory","text":"<p>The <code>models/</code> directory is used to store trained and serialized models. To add this directory to the repository and track its content with Git, please complete the following tasks:</p> <p>TODO: </p> <ul> <li> Add the <code>models/</code> directory to the repository.</li> <li> Create a <code>.gitignore</code> file inside the <code>models/</code> directory</li> <li> Ensure that the content within the <code>models/</code> directory is tracked by Git.</li> </ul>"},{"location":"school/git-101-assignment-1/#5-add-reports-directory","title":"5. Add <code>reports/</code> directory","text":"<p>The <code>reports/</code> directory is intended for storing metrics, generated graphics, and figures used for reporting purposes. To complete the task, follow these steps:</p> <p>TODO: </p> <ul> <li> Add the <code>reports/</code> directory to the repository.</li> <li> Create a <code>.gitignore</code> file inside the <code>reports/</code> directory</li> <li> Ensure that the content within the <code>reports/</code> directory is tracked by Git.</li> </ul>"},{"location":"school/git-101-assignment-1/#6-add-notebooks-directory","title":"6. Add <code>notebooks/</code> directory","text":"<p>The <code>notebooks/</code> directory is used for storing Jupyter Notebook files, often utilized for prototyping purposes. To follow best practices, we should exclude these files from being tracked by Git.</p> <p>TODO:</p> <ul> <li> Add the <code>notebooks/</code> directory to the repository.</li> <li> Exclude the contents of the whole <code>notebooks/</code> directory.  For this update the <code>.gitignore</code> file in the root of your repository</li> <li> <p> But make <code>notebooks**/</code>** directory visible in GitLab remote. To do this you should</p> <ul> <li> create a <code>.gitignore</code> file inside the <code>notebooks/</code> directory.</li> <li> append the same following line to the .gitignore file:</li> </ul> <pre><code>echo '!.gitignore' &gt;&gt; notebooks/.gitignore\n</code></pre> </li> <li> <p> Ensure that the content within the <code>notebooks/</code> directory is not tracked by Git.</p> </li> </ul>"},{"location":"school/git-101-assignment-1/#7-add-the-requirementstxt-file","title":"7. Add the <code>requirements.txt</code> file","text":"<p>It is considered good practice to list all the Python dependencies required for the project in a <code>requirements.txt</code> file. For this template, we will add an empty file.</p> <p>TODO: </p> <ul> <li> Add an empty <code>requirements.txt</code> file to the \u201cproject-1-git\u201d repository.</li> </ul>"},{"location":"school/git-101-assignment-1/#8-add-the-configyaml-file","title":"8.  Add the <code>config.yaml</code> file","text":"<p>Add the <code>config.yaml</code> file in the root of your repository.  The <code>config.yaml</code> file is used to store configuration settings and parameters for a particular application or system.</p> <p>For now <code>config.yaml</code> file will remain empty. </p>"},{"location":"school/git-101-assignment-1/#9-optional-update-the-readmemd-file","title":"9. (optional) Update the <code>README.md</code> file","text":"<p>It is recommended to have a <code>README.md</code> file that provides an overview of the repository, including details about its purpose, structure, and instructions for usage.</p> <p>To do</p> <ul> <li> Create the<code>README.md</code> file</li> </ul>"},{"location":"school/git-101-assignment-1/#10-commit-and-push-updates-to-the-assignment-branch-on-gitlab","title":"10. Commit and Push Updates to the <code>assignment</code> Branch on GitLab","text":"<pre><code>git push origin assignment\n</code></pre>"},{"location":"school/git-101-assignment-1/#11-heck-the-requirements-to-the-assignment","title":"11. \u0421heck the Requirements to the assignment","text":"<p>Check your assignment for the requirements at the beginning of the page. Make sure everything is correct.</p> Success <p>Requirements</p> <p>To ensure the successful completion of the assignment, please make sure the following requirements are met:</p> <ul> <li> The source branch name for the Merge Request is <code>assignment</code>.</li> <li> The <code>data/</code> directory in GitLab only contains a <code>.gitignore</code> file  (files added here are NOT tracked by Git)</li> <li> The <code>models/</code> directory in GitLab has an empty <code>.gitignore</code> file (files added here are tracked by Git)</li> <li> The <code>reports/</code> directory in GitLab has an empty <code>.gitignore</code> (files added here are tracked by Git)</li> <li> The <code>notebooks/</code> directory in GitLab only contains a <code>.gitignore</code> (files added here are NOT tracked by Git)</li> <li> The root of your repository contains an empty <code>config.yaml</code> file.</li> <li> (optional) The README.md file has been updated to provide an overview of the repository, including details about its purpose, structure, and usage instructions.</li> </ul>"},{"location":"school/git-101-assignment-1/#12-submission-create-a-merge-request-to-the-main-branch","title":"12. Submission: Create a Merge Request to the <code>main</code> Branch","text":"<p>Once you have completed all the tasks and are ready to submit your assignment, follow the steps below to create a Merge Request to the <code>main</code> branch:</p> <ol> <li>Go to the GitLab repository page.</li> <li>Click on the \"Merge Requests\" tab.</li> <li>Click on the \"New Merge Request\" button.</li> <li>Set the source branch to <code>assignment</code> and the target branch to <code>main</code>.</li> <li>Provide a title and description for your Merge Request.</li> <li>Review the changes and make sure everything looks correct.</li> <li>Click on the \"Submit Merge Request\" button.</li> </ol> Warning <pre><code>Please DO NOT MERGE your request until you receive a submission report attached to your \nMerge Request.\n\n- Creating a merge request will trigger a CI pipeline that automatically checks our work.\n- A test report will be generated and added to your merge request.\n- This report will contain the results of the automated tests and checks on your assignment.\n- After passing all tests, you may proceed with merging the updates into the `main` branch.\n</code></pre> <p>Submission report looks like this</p> <p></p>"},{"location":"school/git-101-assignment-1/#pro-tips","title":"Pro Tips","text":""},{"location":"school/git-101-assignment-1/#1-use-and-share-the-template","title":"1. Use and share the template","text":"<p>Congratulations on completing the assignment! You have successfully created a Git Repo Template that can be used for new projects. Simply fork or clone the template, rename it, and start using it in your future project</p>"},{"location":"school/git-101-assignment-1/#2-use-vscode-ide-to-generate-gitignore-files","title":"2. Use VSCode IDE to generate <code>.gitignore</code> files","text":"<p>VSCode IDE provides a handy option to generate \u00a0<code>.gitignore</code>\u00a0file for Python language: </p> <ul> <li>press\u00a0<code>CTRL\u00a0+\u00a0Shift\u00a0+\u00a0P</code>\u00a0(<code>CMD\u00a0+\u00a0Shift\u00a0+\u00a0P</code>\u00a0on macOS) to open the\u00a0command palette.</li> <li>type in\u00a0<code>Add gitignore</code>\u00a0in the command palette.</li> <li>Update code &amp; commit</li> </ul> <p></p>"}]}